{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\prototypes\\wave_form_sorter\n",
      "c:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\n",
      "c:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\n",
      "c:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\sequential_axona_sessions\n"
     ]
    }
   ],
   "source": [
    "# Set up paths\n",
    "\n",
    "prototype_dir = os.getcwd()\n",
    "print(prototype_dir)\n",
    "\n",
    "parent = os.path.dirname(prototype_dir)\n",
    "parent_dir = os.path.dirname(parent)\n",
    "sys.path.append(parent_dir)\n",
    "print(parent_dir)\n",
    "\n",
    "top_dir = os.path.dirname(parent_dir)\n",
    "print(top_dir)\n",
    "\n",
    "data_dir = top_dir + r'\\neuroscikit_test_data\\sequential_axona_sessions'\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\prototypes\\wave_form_sorter\n",
      "c:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\prototypes\\wave_form_sorter\n"
     ]
    }
   ],
   "source": [
    "# Set up imports from neuroscikit\n",
    "\n",
    "from core.data_spikes import (\n",
    "    SpikeTrain,\n",
    "    SpikeTrainBatch,\n",
    "    Spike,\n",
    "    SpikeCluster,\n",
    "    SpikeClusterBatch,\n",
    ")\n",
    "\n",
    "from x_io.axona.read_tetrode_and_cut import (\n",
    "    load_spike_train_from_paths,\n",
    "    _read_cut,\n",
    "    _format_spikes\n",
    ")\n",
    "\n",
    "from core.data_study import (\n",
    "    Study,\n",
    "    Event,\n",
    "    Animal\n",
    ")\n",
    "\n",
    "from core.core_utils import (\n",
    "    make_seconds_index_from_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data we are using has two sets of sequential sessions --> extract\n",
    "\n",
    "files = os.listdir(data_dir)\n",
    "\n",
    "test1_34 = []\n",
    "test1_35 = []\n",
    "test2_34 = []\n",
    "test2_35 = []\n",
    "\n",
    "for f in files:\n",
    "    if 'Test1' in f and '34' in f:\n",
    "        test1_34.append(f)\n",
    "    elif 'Test1' in f and '35' in f:\n",
    "        test1_35.append(f)\n",
    "    elif 'Test2' in f and '34' in f:\n",
    "        test2_34.append(f)\n",
    "    elif 'Test2' in f and '35' in f:\n",
    "        test2_35.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tet and cut files from inside folders\n",
    "\n",
    "# session1 = test1_34\n",
    "# session2 = test2_34\n",
    "\n",
    "session1 = test1_35\n",
    "session2 = test2_35\n",
    "\n",
    "assert len(session1) == len(session2)\n",
    "\n",
    "session1_tets = []\n",
    "session2_tets = []\n",
    "\n",
    "for i in range(len(session1)):\n",
    "    if 'cut' in session1[i]:\n",
    "        session1_cut = session1[i]\n",
    "    if 'cut' in session2[i]:\n",
    "        session2_cut = session2[i]\n",
    "    file_session_1 = session1[i]\n",
    "    file_session_2 = session2[i]\n",
    "    out1 = file_session_1.split('.')[-1]\n",
    "    out2 = file_session_2.split('.')[-1]\n",
    "    if out1.isnumeric() and 'clu' not in file_session_1:\n",
    "        session1_tets.append(session1[i])\n",
    "    if out2.isnumeric() and 'clu' not in file_session_2:\n",
    "        session2_tets.append(session2[i])\n",
    "\n",
    "session1_cut_path = os.path.join(data_dir, session1_cut)\n",
    "session1_tet_path = os.path.join(data_dir, session1_tets[0])\n",
    "\n",
    "session2_cut_path = os.path.join(data_dir, session2_cut)\n",
    "session2_tet_path = os.path.join(data_dir, session2_tets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES\n",
    "# \n",
    "# add duration\n",
    "# go overwhich classes use self.timestamp\n",
    "# make Neuron instead of SpikeCluster. Neuron class more extensible, for now merge SpikeTrain fxns, later will want to ahve \n",
    "# summary fxns to see easily if waveform data available or not, maybe make it optional to pass in \n",
    "# check combination of inputs, e.g. if waveforms, need timestamps\n",
    "# e.g. waveforms put in dictionary that holds diff. attributes/stats that can be added to by user --> fxn\n",
    "# also will ahve to add fxn to check that user can add data to the dict\n",
    "\n",
    "# look into error raising that doesnt stop code if exists\n",
    "\n",
    "# spike object --> neuron --> ensemble --> study\n",
    "# ensemble most complex, holds covariates\n",
    "# study ensures conssitency across ensembles, e.g. if one ensemble has this attribute then so do other ensembles\n",
    "# extensible and flexible\n",
    "# study class has functions to return e.g. sorted ensembles/neurons based on added covariate/added attribute\n",
    "\n",
    "# inside file loading classes, make spike objects add to ensemble and in batch_load add ensembles to study\n",
    "\n",
    "# NEw plan: Animal --> Contexts (sess  ions) --> SpikeTrain --> Spike object (event)\n",
    "# Ensemble --> neuron --> spike object\n",
    "\n",
    "# new new plan\n",
    "# Study --> Animal --> Events\n",
    "# Neuron is attribute of event (e.g. spike belonging to a neuron)\n",
    "# polymorphism of events would be spikes (also e.g. lfp)\n",
    "# Ensembles (data structure resulting from group by e.g. spatial, context, maze), can span animal ,session, study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from cut and tet files\n",
    "\n",
    "with open(session1_cut_path, 'r') as cut_file1, open(session1_tet_path, 'rb') as tetrode_file1:\n",
    "    cut_data1 = _read_cut(cut_file1)\n",
    "    tetrode_data1 = _format_spikes(tetrode_file1)\n",
    "    # ts, ch1, ch2, ch3, ch4, spikeparam\n",
    "\n",
    "with open(session2_cut_path, 'r') as cut_file2, open(session2_tet_path, 'rb') as tetrode_file2:\n",
    "    cut_data2 = _read_cut(cut_file2)\n",
    "    tetrode_data2 = _format_spikes(tetrode_file2)\n",
    "    # ts, ch1, ch2, ch3, ch4, spikeparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dictionaries for core classes\n",
    "\n",
    "sample_length1 =  tetrode_data1[-1]['duration']\n",
    "sample_rate1 = tetrode_data1[-1]['samples_per_spike']\n",
    "\n",
    "session_dict1 = {\n",
    "    'spike_times': tetrode_data1[0].squeeze().tolist(),\n",
    "    'cluster_labels': cut_data1,\n",
    "    'ch1': tetrode_data1[1],\n",
    "    'ch2': tetrode_data1[2],\n",
    "    'ch3': tetrode_data1[3],\n",
    "    'ch4': tetrode_data1[4],\n",
    "}\n",
    "\n",
    "sample_length2 =  tetrode_data2[-1]['duration']\n",
    "sample_rate2 = tetrode_data2[-1]['samples_per_spike']\n",
    "\n",
    "session_dict2 = {\n",
    "    'spike_times': tetrode_data2[0].squeeze().tolist(),\n",
    "    'cluster_labels': cut_data2,\n",
    "    'ch1': tetrode_data2[1],\n",
    "    'ch2': tetrode_data2[2],\n",
    "    'ch3': tetrode_data2[3],\n",
    "    'ch4': tetrode_data2[4],\n",
    "}\n",
    "\n",
    "assert sample_length1 == sample_length2\n",
    "assert sample_rate1 == sample_rate2\n",
    "\n",
    "study_dict = {\n",
    "    'sample_length': sample_length1,\n",
    "    'sample_rate': sample_rate1,\n",
    "    'animal_ids': []\n",
    "}\n",
    "\n",
    "animal_dict = {\n",
    "    'id': '0',\n",
    "}\n",
    "\n",
    "animal_dict[0] = session_dict1\n",
    "animal_dict[1] = session_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make study + add animal with sessions, can also add sessions one at a time after making animal instance\n",
    "\n",
    "study = Study(study_dict)\n",
    "\n",
    "study.add_animal(animal_dict)\n",
    "\n",
    "animal = study.animals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_cell(spike_times, cluster_labels, waveforms):\n",
    "    \"\"\"\n",
    "    Takes multiple sessions with spike times, cluster_labels and waveforms.\n",
    "\n",
    "    Returns valid cells for each session and associated waveforms\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(spike_times) == len(cluster_labels)\n",
    "\n",
    "    cells = [[] for i in range(len(spike_times))]\n",
    "    sorted_waveforms = [[] for i in range(len(spike_times))]\n",
    "    good_cells = [[] for i in range(len(spike_times))]\n",
    "    good_sorted_waveforms = [[] for i in range(len(spike_times))]\n",
    "\n",
    "    for i in range(len(spike_times)):\n",
    "        labels = np.unique(cluster_labels[i])\n",
    "        # comes in shape (channel count, spike time, nmb samples) but is nested list not numpy\n",
    "        # want to rearrannge to be (spike time, channel count, nmb sample )\n",
    "        waves = np.array(waveforms[i]).reshape((len(waveforms[i][0]), len(waveforms[i]),  len(waveforms[i][0][0])))\n",
    "        for lbl in labels:\n",
    "            idx = np.where(cluster_labels[i] == lbl)\n",
    "            cells[i].append(np.array(spike_times[i])[idx])\n",
    "            sorted_waveforms[i].append(waves[idx,:,:].squeeze())\n",
    "\n",
    "        empty_cell = 1\n",
    "        for j in range(len(sorted_waveforms[i])):\n",
    "            if len(sorted_waveforms[i][j]) == 0 and j != 0:\n",
    "                empty_cell = j\n",
    "                break\n",
    "            else:\n",
    "                empty_cell = j + 1\n",
    "        for j in range(1,empty_cell,1):\n",
    "            good_cells[i].append(cells[i][j])\n",
    "            good_sorted_waveforms[i].append(sorted_waveforms[i][j])\n",
    "    \n",
    "    return good_cells, good_sorted_waveforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_by_cell(waveforms):\n",
    "    \"\"\"\n",
    "    Averages cell waveforms for each session\n",
    "    \"\"\"\n",
    "    averaged_waveforms = [[] for i in range(len(waveforms))]\n",
    "    # for each session\n",
    "    for i in range(len(waveforms)):\n",
    "        # for each cell\n",
    "        for j in range(len(waveforms[i])):\n",
    "            avg = np.mean(waveforms[i][j].squeeze(), axis=0)\n",
    "            averaged_waveforms[i].append(avg)\n",
    "    return averaged_waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_peaks(x, mph=None, mpd=1, threshold=0, edge='rising',\n",
    "                 kpsh=False, valley=False, show=False, ax=None):\n",
    "    __author__ = \"Marcos Duarte, https://github.com/demotu/BMC\"\n",
    "    __version__ = \"1.0.4\"\n",
    "    __license__ = \"MIT\"\n",
    "\n",
    "    \"\"\"Detect peaks in data based on their amplitude and other features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1D array_like\n",
    "        data.\n",
    "    mph : {None, number}, optional (default = None)\n",
    "        detect peaks that are greater than minimum peak height.\n",
    "    mpd : positive integer, optional (default = 1)\n",
    "        detect peaks that are at least separated by minimum peak distance (in\n",
    "        number of data).\n",
    "    threshold : positive number, optional (default = 0)\n",
    "        detect peaks (valleys) that are greater (smaller) than `threshold`\n",
    "        in relation to their immediate neighbors.\n",
    "    edge : {None, 'rising', 'falling', 'both'}, optional (default = 'rising')\n",
    "        for a flat peak, keep only the rising edge ('rising'), only the\n",
    "        falling edge ('falling'), both edges ('both'), or don't detect a\n",
    "        flat peak (None).\n",
    "    kpsh : bool, optional (default = False)\n",
    "        keep peaks with same height even if they are closer than `mpd`.\n",
    "    valley : bool, optional (default = False)\n",
    "        if True (1), detect valleys (local minima) instead of peaks.\n",
    "    show : bool, optional (default = False)\n",
    "        if True (1), plot data in matplotlib figure.\n",
    "    ax : a matplotlib.axes.Axes instance, optional (default = None).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ind : 1D array_like\n",
    "        indeces of the peaks in `x`.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The detection of valleys instead of peaks is performed internally by simply\n",
    "    negating the data: `ind_valleys = detect_peaks(-x)`\n",
    "\n",
    "    The function can handle NaN's\n",
    "\n",
    "    See this IPython Notebook [1]_.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] http://nbviewer.ipython.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from detect_peaks import detect_peaks\n",
    "    >>> x = np.random.randn(100)\n",
    "    >>> x[60:81] = np.nan\n",
    "    >>> # detect all peaks and plot data\n",
    "    >>> ind = detect_peaks(x, show=True)\n",
    "    >>> print(ind)\n",
    "\n",
    "    >>> x = np.sin(2*np.pi*5*np.linspace(0, 1, 200)) + np.random.randn(200)/5\n",
    "    >>> # set minimum peak height = 0 and minimum peak distance = 20\n",
    "    >>> detect_peaks(x, mph=0, mpd=20, show=True)\n",
    "\n",
    "    >>> x = [0, 1, 0, 2, 0, 3, 0, 2, 0, 1, 0]\n",
    "    >>> # set minimum peak distance = 2\n",
    "    >>> detect_peaks(x, mpd=2, show=True)\n",
    "\n",
    "    >>> x = np.sin(2*np.pi*5*np.linspace(0, 1, 200)) + np.random.randn(200)/5\n",
    "    >>> # detection of valleys instead of peaks\n",
    "    >>> detect_peaks(x, mph=0, mpd=20, valley=True, show=True)\n",
    "\n",
    "    >>> x = [0, 1, 1, 0, 1, 1, 0]\n",
    "    >>> # detect both edges\n",
    "    >>> detect_peaks(x, edge='both', show=True)\n",
    "\n",
    "    >>> x = [-2, 1, -2, 2, 1, 1, 3, 0]\n",
    "    >>> # set threshold = 2\n",
    "    >>> detect_peaks(x, threshold = 2, show=True)\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.atleast_1d(x).astype('float')\n",
    "    if x.size < 3:\n",
    "        return np.array([], dtype=int)\n",
    "    if valley:\n",
    "        x = -x\n",
    "    # find indices of all peaks\n",
    "    dx = x[1:] - x[:-1]\n",
    "    # handle NaN's\n",
    "    indnan = np.where(np.isnan(x))[0]\n",
    "    if indnan.size:\n",
    "        x[indnan] = np.inf\n",
    "        dx[np.where(np.isnan(dx))[0]] = np.inf\n",
    "    ine, ire, ife = np.array([[], [], []], dtype=int)\n",
    "    if not edge:\n",
    "        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "    else:\n",
    "        if edge.lower() in ['rising', 'both']:\n",
    "            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "        if edge.lower() in ['falling', 'both']:\n",
    "            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]\n",
    "    ind = np.unique(np.hstack((ine, ire, ife)))\n",
    "    # handle NaN's\n",
    "    if ind.size and indnan.size:\n",
    "        # NaN's and values close to NaN's cannot be peaks\n",
    "        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan - 1, indnan + 1))), invert=True)]\n",
    "    # first and last values of x cannot be peaks\n",
    "    if ind.size and ind[0] == 0:\n",
    "        ind = ind[1:]\n",
    "    if ind.size and ind[-1] == x.size - 1:\n",
    "        ind = ind[:-1]\n",
    "    # remove peaks < minimum peak height\n",
    "    if ind.size and mph is not None:\n",
    "        ind = ind[x[ind] >= mph]\n",
    "    # remove peaks - neighbors < threshold\n",
    "    if ind.size and threshold > 0:\n",
    "        dx = np.min(np.vstack([x[ind] - x[ind - 1], x[ind] - x[ind + 1]]), axis=0)\n",
    "        ind = np.delete(ind, np.where(dx < threshold)[0])\n",
    "    # detect small peaks closer than minimum peak distance\n",
    "    if ind.size and mpd > 1:\n",
    "        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height\n",
    "        idel = np.zeros(ind.size, dtype=bool)\n",
    "        for i in range(ind.size):\n",
    "            if not idel[i]:\n",
    "                # keep peaks with the same height if kpsh is True\n",
    "                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \\\n",
    "                              & (x[ind[i]] > x[ind] if kpsh else True)\n",
    "                idel[i] = 0  # Keep current peak\n",
    "        # remove the small peaks and sort back the indices by their occurrence\n",
    "        ind = np.sort(ind[~idel])\n",
    "\n",
    "    if show:\n",
    "        if indnan.size:\n",
    "            x[indnan] = np.nan\n",
    "        if valley:\n",
    "            x = -x\n",
    "        # _plot(x, mph, mpd, threshold, edge, valley, ax, ind)\n",
    "\n",
    "    return ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak_amplitudes(waveforms):\n",
    "    \"\"\"\n",
    "    Get peak amplitudes and ids of peaks\n",
    "    \"\"\"\n",
    "    peaks = [[] for i in range(len(waveforms))]\n",
    "    peak_ids = [[] for i in range(len(waveforms))]\n",
    "    for i in range(len(waveforms)):\n",
    "        for j in range(len(waveforms[i])):\n",
    "            cell_peak = []\n",
    "            cell_peak_id = []\n",
    "            for k in range(len(waveforms[i][j])):\n",
    "                cell_peak.append(np.max(waveforms[i][j][k]))\n",
    "                cell_peak_id.append(np.argmax(waveforms[i][j][k]))\n",
    "            peaks[i].append(cell_peak)\n",
    "            peak_ids[i].append(cell_peak_id)\n",
    "    return peaks, peak_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spike_width(cell_waveforms, sample_rate):\n",
    "    \"\"\"\n",
    "    Get spike width data for one cell\n",
    "\n",
    "    input shape is (num spikes, num channel, num samples per waveform)\n",
    "    \"\"\"\n",
    "    avg_waveform = np.mean(cell_waveforms, axis=0)\n",
    "\n",
    "    pp_amp = np.zeros((cell_waveforms.shape[0], cell_waveforms.shape[1]))\n",
    "    pt_width = np.zeros_like(pp_amp)\n",
    "    peak_cell = np.zeros_like(pp_amp)\n",
    "    # aup = np.zeros_like(pp_amp)\n",
    "\n",
    "    reshaped_waveforms = np.zeros((cell_waveforms.shape[1], cell_waveforms.shape[0], cell_waveforms.shape[2]))\n",
    "    for i in range(len(cell_waveforms)):\n",
    "        for j in range(len(cell_waveforms[i])):\n",
    "            reshaped_waveforms[j,i] = cell_waveforms[i,j]\n",
    "    cell_waveforms = reshaped_waveforms\n",
    "\n",
    "    for chan_index, channel in enumerate(cell_waveforms):\n",
    "\n",
    "        for spike_index, spike in enumerate(channel):\n",
    "            # aup[spike_index, chan_index] = AUP(spike, t_peak)\n",
    "\n",
    "            t = ((10 ** 6) / sample_rate) * np.arange(0, len(spike))\n",
    "\n",
    "            locs = detect_peaks(spike, edge='both', threshold=0)\n",
    "\n",
    "            pks = spike[locs]\n",
    "\n",
    "            if len(pks) == 0:\n",
    "                peak_cell[spike_index, :] = np.NaN\n",
    "\n",
    "            else:\n",
    "                max_ind = np.where(pks == np.amax(pks))[0]  # finding the index of the max value\n",
    "                locs_ts = locs * (10 ** 6. / sample_rate)  # converting the peak locations to seconds\n",
    "                min_val = np.amin(spike[np.where(t > locs_ts[max_ind[0]])[0]])  # finding the minimum value\n",
    "                min_ind = np.where((spike == min_val) & (t > locs_ts[max_ind[0]]))[0][\n",
    "                    0]  # finding the index of the minimum value\n",
    "                min_t = t[min_ind]  # converting the index to a time value\n",
    "                peak_cell[spike_index, :] = np.array([np.amax(pks), locs_ts[max_ind[0]], min_val, min_t])\n",
    "\n",
    "            # if sum(np.isnan(peak_cell[:, 0])) != 0:\n",
    "            #     print('waveform with no peaks')\n",
    "            #     print(chan_index, locs)\n",
    "            #     print(spike_index, spike)\n",
    "            #     print(channel[spike_index])\n",
    "            #     sto()\n",
    "\n",
    "            pp_amp[:, chan_index] = peak_cell[:, 0] - peak_cell[:, 2]\n",
    "            pt_width[:, chan_index] = peak_cell[:, 1] - peak_cell[:, 3]\n",
    "\n",
    "            # ------------ Done calculating the peak to peaks and peak to through values ------------- #\n",
    "\n",
    "        avg_pp_amp = np.zeros((1, len(cell_waveforms)))\n",
    "        avg_pt_width = np.zeros_like(avg_pp_amp)\n",
    "        # avg_aup = np.zeros_like(avg_pp_amp)\n",
    "\n",
    "        for waveform_index, waveform in enumerate(avg_waveform):\n",
    "            locs = detect_peaks(waveform, edge='both', threshold=0)\n",
    "            locs_ts = locs * (10 ** 6. / sample_rate)  # converting the peak locations to seconds\n",
    "            pks = waveform[locs]\n",
    "            max_val = np.amax(pks)\n",
    "            max_ind = np.where(pks == max_val)[0]  # finding the index of the max value\n",
    "            max_t = locs_ts[max_ind[0]]\n",
    "            min_val = np.amin(waveform[np.where(t > locs_ts[max_ind[0]])[0]])  # finding the minimum value\n",
    "            min_ind = np.where(waveform == min_val)[0]  # finding the index of the minimum value\n",
    "            min_t = t[min_ind]  # converting the index to a time value\n",
    "            avg_pp_amp[0, waveform_index] = max_val - min_val\n",
    "            avg_pt_width[0, waveform_index] = min_t - max_t\n",
    "            # avg_aup[0, waveform_index] = AUP(waveform, t_peak)\n",
    "    # ------------ ended avg_waveform calculations ------------------- #\n",
    "    best_channel = np.where(avg_pp_amp == np.amax(avg_pp_amp))[1][0]\n",
    "\n",
    "    channel = 'ch%d' % int(best_channel + 1)\n",
    "    waveform_dict = {}\n",
    "    for channel_index, channel_waveform in enumerate(cell_waveforms):\n",
    "        waveform_dict['ch%d' % int(channel_index + 1)] = channel_waveform\n",
    "    waveform_dict.update({'sample_rate': sample_rate, 'pp_amp': avg_pp_amp[0],\n",
    "                          'spike_width': avg_pt_width[0], 'peak_channel': best_channel})\n",
    "    return waveform_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_waveforms_by_sesssion(animal: Animal):\n",
    "    \"\"\"\n",
    "    Sort waveforms by session\n",
    "    \"\"\"\n",
    "    cells, sorted_waveforms = sort_by_cell(animal.agg_spike_times, animal.agg_cluster_labels, animal.agg_waveforms)\n",
    "\n",
    "    waveforms = average_by_cell(sorted_waveforms)\n",
    "\n",
    "    # peaks = [[] for i in range(len(waveforms))]\n",
    "    # for i in range(len(waveforms)):\n",
    "    #     for j in range(len(waveforms[i])):\n",
    "    #         cell_peak = []\n",
    "    #         for k in range(len(waveforms[i][j])):\n",
    "    #             cell_peak.append(detect_peaks(waveforms[i][j][k]))\n",
    "    #         peaks[i].append(cell_peak)\n",
    "\n",
    "    peaks, peak_ids = get_peak_amplitudes(waveforms)\n",
    "\n",
    "    agg_waveform_dict = {}\n",
    "    for i in range(len(sorted_waveforms)):\n",
    "        session_key = 'session_' + str(i+1)\n",
    "        agg_waveform_dict[session_key] = {}\n",
    "        for j in range(len(sorted_waveforms[i])):\n",
    "            waveform_dict = get_spike_width(sorted_waveforms[i][j], study.sample_rate)\n",
    "            orders = get_possible_orders(waveform_dict['pp_amp'], threshold=0.2)\n",
    "            waveform_dict['channel_orders'] = orders\n",
    "            cell_key = 'cell_' + str(j+1)\n",
    "            agg_waveform_dict[session_key][cell_key] = waveform_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vectors(spike_amps, spike_widths, peak_channels):\n",
    "    \"\"\"\n",
    "    Vectors will be in format [[start x, start y], [end x, end y]]\n",
    "    \"\"\"\n",
    "\n",
    "    vectors = [[] for i in range(len(spike_amps))]\n",
    "\n",
    "    tet_x_coord = [-1,1,1,-1]\n",
    "    tet_y_coord = [1,1,-1,-1]\n",
    "\n",
    "    for i in range(len(spike_amps)):\n",
    "        ses_amps = spike_amps[i]\n",
    "        mn = np.mean(ses_amps)\n",
    "        std = np.std(ses_amps)\n",
    "        for j in range(len(ses_amps)):\n",
    "            cell_amps = (ses_amps[j] - mn)/std\n",
    "            cell_widths = spike_widths[i][j]\n",
    "            cell_peak = peak_channels[i][j]\n",
    "\n",
    "            loc_x = np.sum(np.array(tet_x_coord) * np.array(cell_amps))\n",
    "            loc_y = np.sum(np.array(tet_y_coord) * np.array(cell_amps))\n",
    "\n",
    "            vector = [loc_x, loc_y, tet_x_coord[cell_peak], tet_y_coord[cell_peak]]\n",
    "            # vector = [0, 0, loc_x, loc_y]\n",
    "            vectors[i].append(vector)\n",
    "            \n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_switch_channels(pp_amp, threshold):\n",
    "    \"\"\"\n",
    "    For session for cell, get channel in tetrode that could switch in a possible ordering\n",
    "    \"\"\"\n",
    "    can_switch = []\n",
    "    for i in range(len(pp_amp)):\n",
    "        for j in range(len(pp_amp)):\n",
    "            pair = np.sort([i,j]).tolist()\n",
    "            if i != j and pair not in can_switch:\n",
    "                diff = abs(pp_amp[i] - pp_amp[j])\n",
    "                if diff <= threshold:\n",
    "                    can_switch.append(pair)\n",
    "    return can_switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_orders(pp_amp, threshold=.2):\n",
    "    \"\"\" \n",
    "    Get possible ordering for a cell\n",
    "    \"\"\"\n",
    "    # avg_waveform = np.mean(cell_waveforms, axis=0)\n",
    "    possible_orders = []\n",
    "\n",
    "    can_switch = get_switch_channels(pp_amp, threshold)\n",
    "    switched = []\n",
    "    true_order = np.argsort(pp_amp)\n",
    "    possible_orders.append(true_order.tolist())\n",
    "    \n",
    "    for i in range(len(pp_amp)):\n",
    "        for j in range(len(can_switch)):\n",
    "            if i in can_switch[j] and can_switch[j] not in switched:\n",
    "                new_order = np.copy(true_order)\n",
    "                id1 = np.where(new_order == can_switch[j][0])[0]\n",
    "                id2 = np.where(new_order == can_switch[j][1])[0]\n",
    "                new_order[id1] = can_switch[j][1]\n",
    "                new_order[id2] = can_switch[j][0]\n",
    "                switched.append(can_switch[j])\n",
    "                possible_orders.append(new_order.tolist())\n",
    "\n",
    "    return possible_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cells, sorted_waveforms = sort_by_cell(animal.agg_spike_times, animal.agg_cluster_labels, animal.agg_waveforms)\n",
    "waveforms = average_by_cell(sorted_waveforms)\n",
    "peaks, peak_ids = get_peak_amplitudes(waveforms)\n",
    "\n",
    "agg_waveform_dict = {}\n",
    "for i in range(len(sorted_waveforms)):\n",
    "    session_key = 'session_' + str(i+1)\n",
    "    agg_waveform_dict[session_key] = {}\n",
    "    for j in range(len(sorted_waveforms[i])):\n",
    "        waveform_dict = get_spike_width(sorted_waveforms[i][j], study.sample_rate)\n",
    "        orders = get_possible_orders(waveform_dict['pp_amp'], threshold=0.2)\n",
    "        waveform_dict['channel_orders'] = orders\n",
    "        cell_key = 'cell_' + str(j+1)\n",
    "        agg_waveform_dict[session_key][cell_key] = waveform_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit\\prototypes\\wave_form_sorter\\wave_form_sorter.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aaoun/OneDrive%20-%20cumc.columbia.edu/Desktop/HussainiLab/neuroscikit/prototypes/wave_form_sorter/wave_form_sorter.ipynb#Y200sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m             matched\u001b[39m.\u001b[39mappend([\u001b[39mint\u001b[39m(i\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), \u001b[39mint\u001b[39m(j\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aaoun/OneDrive%20-%20cumc.columbia.edu/Desktop/HussainiLab/neuroscikit/prototypes/wave_form_sorter/wave_form_sorter.ipynb#Y200sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(cell_1[\u001b[39m0\u001b[39m]) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mtype\u001b[39m(cell_2[\u001b[39m0\u001b[39m]) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aaoun/OneDrive%20-%20cumc.columbia.edu/Desktop/HussainiLab/neuroscikit/prototypes/wave_form_sorter/wave_form_sorter.ipynb#Y200sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mif\u001b[39;00m cell_1\u001b[39m.\u001b[39;49mall() \u001b[39m==\u001b[39m cell_2\u001b[39m.\u001b[39mall():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aaoun/OneDrive%20-%20cumc.columbia.edu/Desktop/HussainiLab/neuroscikit/prototypes/wave_form_sorter/wave_form_sorter.ipynb#Y200sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         matched\u001b[39m.\u001b[39mappend([\u001b[39mint\u001b[39m(i\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), \u001b[39mint\u001b[39m(j\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'all'"
     ]
    }
   ],
   "source": [
    "# Check if possible orders for each cell match across sessions\n",
    "\n",
    "session_1 = agg_waveform_dict['session_1']\n",
    "session_2 = agg_waveform_dict['session_2']\n",
    "\n",
    "matched = []\n",
    "\n",
    "for i in session_1:\n",
    "    for j in session_2:\n",
    "        cell_1 = session_1[i]['channel_orders']\n",
    "        cell_2 = session_2[j]['channel_orders']\n",
    "        if len(cell_1) > 0 and len(cell_2) > 0:\n",
    "            if type(cell_1[0]) == list and type(cell_2[0]) == list:\n",
    "                for k in range(len(cell_1)):\n",
    "                    if cell_1[k] in cell_2:\n",
    "                        matched.append([int(i.split('_')[-1]), int(j.split('_')[-1])])\n",
    "            elif type(cell_1[0]) == list and type(cell_2[0]) != list:\n",
    "                for k in range(len(cell_1)):\n",
    "                    if cell_1[k] == cell_2:\n",
    "                        matched.append([int(i.split('_')[-1]), int(j.split('_')[-1])])\n",
    "            elif type(cell_1[0]) != list and type(cell_2[0]) == list:\n",
    "                for k in range(len(cell_2)):\n",
    "                    if cell_1 == cell_2[k]:\n",
    "                        matched.append([int(i.split('_')[-1]), int(j.split('_')[-1])])\n",
    "            elif type(cell_1[0]) != list and type(cell_2[0]) != list:\n",
    "                if cell_1 == cell_2:\n",
    "                    matched.append([int(i.split('_')[-1]), int(j.split('_')[-1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "# dim1 = len(session_1)\n",
    "# dim2 = len(session_2)\n",
    "\n",
    "# axs = []\n",
    "\n",
    "# session_1_pos = [1,3,5,7,9]\n",
    "# # 11,13,15,17,19]\n",
    "# session_2_pos = [2,4,6,8,10]\n",
    "# # 12,14,16,18,20]\n",
    "\n",
    "\n",
    "# for i in range(dim1):\n",
    "#     ax = plt.subplot(dim1+dim2, 2, session_1_pos[i])\n",
    "#     cell_dict = session_1['cell_'+str(i+1)]\n",
    "#     for key in cell_dict:\n",
    "#         if key.split('ch')[-1].isnumeric():\n",
    "#             ax.plot(np.mean(cell_dict[key], axis=0), label=key)\n",
    "#             # ax.legend()\n",
    "#     axs.append(ax)\n",
    "    \n",
    "#     ax = plt.subplot(dim1+dim2, 2, session_2_pos[i])\n",
    "#     cell_dict = session_2['cell_'+str(i+1)]\n",
    "#     for key in cell_dict:\n",
    "#         if key.split('ch')[-1].isnumeric():\n",
    "#             ax.plot(np.mean(cell_dict[key], axis=0), label=key)\n",
    "#             # ax.legend()\n",
    "#     axs.append(ax)\n",
    "\n",
    "# for ax in axs:\n",
    "#     ax.set_ylabel('Waveform')\n",
    "#     ax.set_xlabel('Sample')\n",
    "    \n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate spike width and amp\n",
    "\n",
    "agg_spike_amp = [[] for i in range(len(agg_waveform_dict))]\n",
    "agg_spike_width = [[] for i in range(len(agg_waveform_dict))]\n",
    "agg_peak_channel = [[] for i in range(len(agg_waveform_dict))]\n",
    "\n",
    "c = 0\n",
    "for session_key in agg_waveform_dict:\n",
    "    session = agg_waveform_dict[session_key]\n",
    "    for cell_key in session:\n",
    "        cell = session[cell_key]\n",
    "        agg_spike_amp[c].append(cell['pp_amp'])\n",
    "        agg_spike_width[c].append(cell['spike_width'])\n",
    "        agg_peak_channel[c].append(cell['peak_channel'])\n",
    "    c += 1\n",
    "\n",
    "agg_spike_amp = np.array(agg_spike_amp)\n",
    "agg_spike_width = np.array(agg_spike_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = make_vectors(agg_spike_amp, agg_spike_width, agg_peak_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "ax1 = plt.subplot(1,3,1)\n",
    "ax2 = plt.subplot(1,3,2)\n",
    "ax3 = plt.subplot(1,3,3)\n",
    "axs = [ax1, ax2, ax3]\n",
    "labels = ['Session 1', 'Session 2']\n",
    "colors = ['r', 'g']\n",
    "\n",
    "for i in range(len(vectors)):\n",
    "    for j in range(len(vectors[i])):\n",
    "        vector = vectors[i][j]\n",
    "        axs[i].quiver(vector[0], vector[1], vector[2], vector[3], color='r')\n",
    "\n",
    "        if j == 0:\n",
    "            axs[-1].quiver(vector[0], vector[1], vector[2], vector[3], color=colors[i], label=labels[i])\n",
    "        else:\n",
    "            axs[-1].quiver(vector[0], vector[1], vector[2], vector[3], color=colors[i])\n",
    "\n",
    "for ax in axs:\n",
    "    # center\n",
    "    ax.plot(0, 0, 'kx')\n",
    "    # tetrode 1\n",
    "    ax.plot(-1, 1, 'ko')\n",
    "    # tetrode 2\n",
    "    ax.plot(1, 1, 'ko')\n",
    "    # tetrode 1\n",
    "    ax.plot(1, -1, 'ko')\n",
    "    # tetrode 1\n",
    "    ax.plot(-1, -1, 'ko')\n",
    "\n",
    "    ax.set_ylim(-2.5,2.5)\n",
    "    ax.set_xlim(-2.5,2.5)\n",
    "\n",
    "axs[-1].legend()\n",
    "axs[0].set_title(labels[0])\n",
    "axs[1].set_title(labels[1])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "# tet_coords = [[0,0], [-1,1], [1,1], [1,-1]]\n",
    "tet_coords = [[-1,-1,1,1], [-1,1,1,-1]]\n",
    "\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "\n",
    "mean_amp = np.mean(agg_spike_amp[0])\n",
    "std_amp = np.std(agg_spike_amp[0])\n",
    "mean_wid = np.mean(agg_spike_width[0])\n",
    "std_wid = np.std(agg_spike_width[0])\n",
    "for i in range(len(agg_spike_amp[0])):\n",
    "    tet_amp = (agg_spike_amp[0][i] - mean_amp)/std_amp\n",
    "    tet_width = (agg_spike_width[0][i] - mean_wid)/std_wid\n",
    "\n",
    "    to_plot_x = np.array(tet_coords[0]) * np.array(tet_amp) \n",
    "    to_plot_y = np.array(tet_coords[1]) * np.array(tet_amp) \n",
    "    to_plot = [sum(to_plot_x), sum(to_plot_y)]\n",
    "\n",
    "    ax1.plot(to_plot[0], to_plot[1], 'r*')\n",
    "    # print(to_plot)\n",
    "    # ax1.quiver(np.array([0,0]), to_plot)\n",
    "\n",
    "ax1.set_title('session 1')\n",
    "\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "\n",
    "mean_amp = np.mean(agg_spike_amp[1])\n",
    "std_amp = np.std(agg_spike_amp[1])\n",
    "mean_wid = np.mean(agg_spike_width[1])\n",
    "std_wid = np.std(agg_spike_width[1])\n",
    "for i in range(len(agg_spike_amp[1])):\n",
    "    tet_amp = (agg_spike_amp[1][i] - mean_amp)/std_amp\n",
    "    tet_width = (agg_spike_width[1][i] - mean_wid)/std_wid\n",
    "\n",
    "    to_plot_x = np.array(tet_coords[0]) * np.array(tet_amp) \n",
    "    to_plot_y = np.array(tet_coords[1]) * np.array(tet_amp) \n",
    "    to_plot = [sum(to_plot_x), sum(to_plot_y)]\n",
    "\n",
    "    ax2.plot(to_plot[0], to_plot[1], 'r*')\n",
    "\n",
    "ax2.set_title('session 2')\n",
    "\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "mean_amp = np.mean(agg_spike_amp[0])\n",
    "std_amp = np.std(agg_spike_amp[0])\n",
    "mean_wid = np.mean(agg_spike_width[0])\n",
    "std_wid = np.std(agg_spike_width[0])\n",
    "for i in range(len(agg_spike_amp[0])):\n",
    "    tet_amp = (agg_spike_amp[0][i] - mean_amp)/std_amp\n",
    "    tet_width = (agg_spike_width[0][i] - mean_wid)/std_wid\n",
    "    \n",
    "    to_plot_x = np.array(tet_coords[0]) * np.array(tet_amp) \n",
    "    to_plot_y = np.array(tet_coords[1]) * np.array(tet_amp) \n",
    "    to_plot = [sum(to_plot_x), sum(to_plot_y)]\n",
    "\n",
    "    if i == 0:\n",
    "        ax3.plot(to_plot[0], to_plot[1], 'r*', label='session 1')\n",
    "    else:\n",
    "        ax3.plot(to_plot[0], to_plot[1], 'r*')\n",
    "    \n",
    "mean_amp = np.mean(agg_spike_amp[1])\n",
    "std_amp = np.std(agg_spike_amp[1])\n",
    "mean_wid = np.mean(agg_spike_width[1])\n",
    "std_wid = np.std(agg_spike_width[1])\n",
    "for i in range(len(agg_spike_amp[1])):\n",
    "    tet_amp = (agg_spike_amp[1][i] - mean_amp)/std_amp\n",
    "    tet_width = (agg_spike_width[1][i] - mean_wid)/std_wid\n",
    "    \n",
    "    to_plot_x = np.array(tet_coords[0]) * np.array(tet_amp) \n",
    "    to_plot_y = np.array(tet_coords[1]) * np.array(tet_amp) \n",
    "    to_plot = [sum(to_plot_x), sum(to_plot_y)]\n",
    "\n",
    "    if i == 0:\n",
    "        ax3.plot(to_plot[0], to_plot[1], 'g*', label='session 2')\n",
    "    else:\n",
    "        ax3.plot(to_plot[0], to_plot[1], 'g*')\n",
    "\n",
    "ax3.set_title('Both sessions')\n",
    "ax3.legend()\n",
    "\n",
    "axs = [ax1, ax2, ax3]\n",
    "\n",
    "for ax in axs:\n",
    "    # center\n",
    "    ax.plot(0, 0, 'kx')\n",
    "    # tetrode 1\n",
    "    ax.plot(-1, 1, 'ko')\n",
    "    # tetrode 2\n",
    "    ax.plot(1, 1, 'ko')\n",
    "    # tetrode 1\n",
    "    ax.plot(1, -1, 'ko')\n",
    "    # tetrode 1\n",
    "    ax.plot(-1, -1, 'ko')\n",
    "\n",
    "    ax.set_ylim(-3,3)\n",
    "    ax.set_xlim(-3,3)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peaks = [[] for i in range(len(waveforms))]\n",
    "# for i in range(len(waveforms)):\n",
    "#     for j in range(len(waveforms[i])):\n",
    "#         cell_peak = []\n",
    "#         for k in range(len(waveforms[i][j])):\n",
    "#             cell_peak.append(np.argmax(waveforms[i][j][k]))\n",
    "#         peaks[i].append(cell_peak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import struct, os\n",
    "import numpy.matlib\n",
    "from scipy.io import savemat\n",
    "import mmap\n",
    "import contextlib\n",
    "\n",
    "\n",
    "def int16toint8(value):\n",
    "    \"\"\"Converts int16 data to int8\"\"\"\n",
    "    value = np.divide(value, 256).astype(int)\n",
    "    value[np.where(value > 127)] = 127\n",
    "    value[np.where(value < -128)] = -128\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def remEEGShift(t, EEG):\n",
    "    \"\"\"This function removes the shift that the EEG undergoes\"\"\"\n",
    "    EEG = EEG.flatten()\n",
    "    Fs_EGF = int(4.8e3)\n",
    "    # normal delay is 0.5(n_taps-1) / Fs_EGF, n_taps is 101 for the FIR filter\n",
    "    # there is already a shift of 18 as the EEG starts off using the 18th EGF data\n",
    "    delay = (0.5 * (101 - 1) - 18) / Fs_EGF\n",
    "\n",
    "    t = t - delay\n",
    "\n",
    "    t_bool = np.where(t >= 0)[0]\n",
    "    t = t[t_bool]\n",
    "    EEG = EEG[t_bool]\n",
    "\n",
    "    return t, EEG\n",
    "\n",
    "\n",
    "def MatlabNumSeq(start, stop, step, exclude=True):\n",
    "    \"\"\"In Matlab you can type:\n",
    "\n",
    "    start:step:stop and easily create a numerical sequence\n",
    "\n",
    "    if exclude is true it will exclude any values greater than the stop value\n",
    "    \"\"\"\n",
    "\n",
    "    '''np.arange(start, stop, step) works good most of the time\n",
    "\n",
    "    However, if the step (stop-start)/step is an integer, then the sequence\n",
    "    will stop early'''\n",
    "\n",
    "    seq = np.arange(start, stop + step, step)\n",
    "\n",
    "    if exclude:\n",
    "        if seq[-1] > stop:\n",
    "            seq = seq[:-1]\n",
    "\n",
    "    return seq\n",
    "\n",
    "\n",
    "class TintException(Exception):\n",
    "    def __init___(self, message):\n",
    "        Exception.__init__(self, \"%s\" % message)\n",
    "        self.message = message\n",
    "\n",
    "\n",
    "def get_good_cells(units):\n",
    "    \"\"\"\n",
    "    In Tint we have a method of determining the 'good cells'. A good tetrode must begin\n",
    "    with cell #1, and the set of consecutive cell numberes are all good. Essentially we leave\n",
    "    a blank cell separating the good cells from the bad. This function will return the good cells.\n",
    "    \"\"\"\n",
    "    units = np.unique(units)\n",
    "\n",
    "    if 1 not in units:\n",
    "        return np.array([])\n",
    "\n",
    "    good_cells = [1]\n",
    "\n",
    "    found_separation = False\n",
    "    for index, unit in enumerate(units):\n",
    "        if unit > 1:\n",
    "            if unit == units[index - 1] + 1 and not found_separation:\n",
    "                good_cells.append(unit)\n",
    "            else:  #\n",
    "                # no longer consecutive units, so set value to false to ensure no more appending\n",
    "                found_separation = True\n",
    "\n",
    "    return np.asarray(good_cells)\n",
    "\n",
    "\n",
    "def get_setfile_parameter(parameter, set_filename):\n",
    "    \"\"\"\n",
    "    This function will return the parameter value of a given parameter name for a given set filename.\n",
    "\n",
    "    Example:\n",
    "        set_fullpath = 'C:\\\\example\\\\tetrode_1.1'\n",
    "        parameter_name = 'duration\n",
    "        duration = get_setfile_parameter(parameter_name, set_fullpath)\n",
    "\n",
    "    Args:\n",
    "        parameter (str): the name of the set file parameter that you want to obtain.\n",
    "        set_filename (str): the full path of the .set file that you want to obtain the parameter value from.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        parameter_value (str): the value for the given parameter\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(set_filename):\n",
    "        return\n",
    "\n",
    "    # adding the encoding because tint data is created via windows and if you want to run this in linux, you need\n",
    "    # to explicitly say this\n",
    "    with open(set_filename, 'r+', encoding='cp1252') as f:\n",
    "        for line in f:\n",
    "            if parameter in line:\n",
    "                if line.split(' ')[0] == parameter:\n",
    "                    # prevents part of the parameter being in another parameter name\n",
    "                    new_line = line.strip().split(' ')\n",
    "                    if len(new_line) == 2:\n",
    "                        return new_line[-1]\n",
    "                    else:\n",
    "                        return ' '.join(new_line[1:])\n",
    "\n",
    "\n",
    "def getpos(position_filename, ppm=None, center=None, method=None, flip_y=True):\n",
    "    \"\"\"\n",
    "    This method will allow you to provide a position filename and a few parameters to obtain the position values.\n",
    "\n",
    "    :param position_filename: filename value to your .pos file: 'c:\\\\example_session.pos'\n",
    "    :param ppm: pixels per meter value to convert the data that is stored in bits, to a meter (or centimeter) value.\n",
    "    :param center: the center point of the arena [center_x, center_y]\n",
    "    :param method: I allow a method function so that I can return the raw values if I want. To receive the values as\n",
    "        the data was saved, use method='raw'.\n",
    "    :param flip_y: this will flip the y values as is done in Tint\n",
    "    :return: x, y, t, Fs\n",
    "    \"\"\"\n",
    "    num_pos_samples = None\n",
    "    x = None\n",
    "    y = None\n",
    "    t = None\n",
    "    Fs = None\n",
    "    timebase = None\n",
    "    two_spot = True\n",
    "    with open(position_filename, 'rb+') as f:  # opening the .pos file\n",
    "        headers = ''  # initializing the header string\n",
    "        for line in f:  # reads line by line to read the header of the file\n",
    "            # print(line)\n",
    "            if 'data_start' in str(line):  # if it reads data_start that means the header has ended\n",
    "                headers += 'data_start'\n",
    "                break  # break out of for loop once header has finished\n",
    "            elif 'duration' in str(line):\n",
    "                headers += line.decode(encoding='UTF-8')\n",
    "            elif 'num_pos_samples' in str(line):\n",
    "                num_pos_samples = int(line.decode(encoding='UTF-8')[len('num_pos_samples '):])\n",
    "                headers += line.decode(encoding='UTF-8')\n",
    "            elif 'bytes_per_timestamp' in str(line):\n",
    "                # bytes_per_timestamp = int(line.decode(encoding='UTF-8')[len('bytes_per_timestamp '):])\n",
    "                headers += line.decode(encoding='UTF-8')\n",
    "            elif 'bytes_per_coord' in str(line):\n",
    "                # bytes_per_coord = int(line.decode(encoding='UTF-8')[len('bytes_per_coord '):])\n",
    "                headers += line.decode(encoding='UTF-8')\n",
    "            elif 'timebase' in str(line):\n",
    "                timebase = (line.decode(encoding='UTF-8')[len('timebase '):]).split(' ')[0]\n",
    "                headers += line.decode(encoding='UTF-8')\n",
    "            elif 'pixels_per_metre' in str(line):\n",
    "                if ppm is None:\n",
    "                    ppm = float(line.decode(encoding='UTF-8')[len('pixels_per_metre '):])\n",
    "                headers += line.decode(encoding='UTF-8')\n",
    "            elif 'min_x' in str(line) and 'window' not in str(line):\n",
    "                # min_x = int(line.decode(encoding='UTF-8')[len('min_x '):])\n",
    "                headers += line.decode(encoding='UTF-8')\n",
    "            elif 'max_x' in str(line) and 'window' not in str(line):\n",
    "                # max_x = int(line.decode(encoding='UTF-8')[len('max_x '):])\n",
    "                headers += line.decode(encoding='UTF-8')\n",
    "            elif 'min_y' in str(line) and 'window' not in str(line):\n",
    "                # min_y = int(line.decode(encoding='UTF-8')[len('min_y '):])\n",
    "                headers += line.decode(encoding='UTF-8')\n",
    "            elif 'max_y' in str(line) and 'window' not in str(line):\n",
    "                # max_y = int(line.decode(encoding='UTF-8')[len('max_y '):])\n",
    "                headers += line.decode(encoding='UTF-8')\n",
    "            elif 'pos_format' in str(line):\n",
    "                headers += line.decode(encoding='UTF-8')\n",
    "                if 't,x1,y1,x2,y2,numpix1,numpix2' in str(line):\n",
    "                    two_spot = True\n",
    "                else:\n",
    "                    two_spot = False\n",
    "                    print('The position format is unrecognized!')\n",
    "\n",
    "            elif 'sample_rate' in str(line):\n",
    "                Fs = float(line.decode(encoding='UTF-8').split(' ')[1])\n",
    "                headers += line.decode(encoding='UTF-8')\n",
    "\n",
    "            else:\n",
    "                headers += line.decode(encoding='UTF-8')\n",
    "\n",
    "    # currently we only really use the two_spot mode from Tint, so that is all that is coded\n",
    "    if two_spot:\n",
    "        '''Run when two spot mode is on, (one_spot has the same format so it will also run here)'''\n",
    "        with open(position_filename, 'rb+') as f:\n",
    "            '''get_pos for one_spot'''\n",
    "            pos_data = f.read()  # all the position data values (including header)\n",
    "            pos_data = pos_data[len(headers):-12]  # removes the header values\n",
    "\n",
    "            byte_string = 'i8h'\n",
    "\n",
    "            pos_data = np.asarray(struct.unpack('>%s' % (num_pos_samples * byte_string), pos_data))\n",
    "            pos_data = pos_data.astype(float).reshape((num_pos_samples, 9))  # there are 8 words and 1 time sample\n",
    "\n",
    "        x = pos_data[:, 1]\n",
    "        y = pos_data[:, 2]\n",
    "        t = pos_data[:, 0]\n",
    "\n",
    "        x = x.reshape((len(x), 1))\n",
    "        y = y.reshape((len(y), 1))\n",
    "        t = t.reshape((len(t), 1))\n",
    "\n",
    "        if method is not None:\n",
    "            if method == 'raw':\n",
    "                return x, y, t, Fs\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        t = np.divide(t, np.float(timebase))  # converting the frame number from Axona to the time value\n",
    "\n",
    "        # values that are NaN are set to 1023 in Axona's system, replace these values by NaN's\n",
    "\n",
    "        x[np.where(x == 1023)] = np.nan\n",
    "        y[np.where(y == 1023)] = np.nan\n",
    "\n",
    "        didFix, fixedPost = fixTimestamps(t)\n",
    "\n",
    "        if didFix:\n",
    "            t = fixedPost\n",
    "\n",
    "        t = t - t[0]\n",
    "\n",
    "        # if there was some timestamp fixing, then the length of the time array could be less than that of the x and y\n",
    "        # check that here and correct lengths\n",
    "        if len(t) != len(x):\n",
    "            x = x[:len(t)]\n",
    "            y = y[:len(t)]\n",
    "\n",
    "        # convert the data to cm\n",
    "        x, y = arena_config(x, y, ppm, center=center, flip_y=flip_y)\n",
    "\n",
    "        # remove any NaNs at the end of the file\n",
    "        x, y, t = removeNan(x, y, t)\n",
    "\n",
    "    else:\n",
    "        print(\"Haven't made any code for this part yet.\")\n",
    "\n",
    "    return x.reshape((len(x), 1)), y.reshape((len(y), 1)), t.reshape((len(t), 1)), Fs\n",
    "\n",
    "\n",
    "def is_tetrode(file, session):\n",
    "    \"\"\"\"\n",
    "    Determines if the file is a tetrode, essentially will look at the extension and see if it ends in an integer\n",
    "    or not. Also checks that the tetrode belongs to a given session.\n",
    "    \"\"\"\n",
    "    if os.path.splitext(file)[0] == session:\n",
    "        try:\n",
    "            tetrode_number = int(os.path.splitext(file)[1][1:])\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_active_tetrode(set_filename):\n",
    "    \"\"\"in the .set files it will say collectMask_X Y for each tetrode number to tell you if\n",
    "    it is active or not. T1 = ch1-ch4, T2 = ch5-ch8, etc.\"\"\"\n",
    "    active_tetrode = []\n",
    "    active_tetrode_str = 'collectMask_'\n",
    "\n",
    "    with open(set_filename) as f:\n",
    "        for line in f:\n",
    "\n",
    "            # collectMask_X Y, where x is the tetrode number, and Y is eitehr on or off (1 or 0)\n",
    "            if active_tetrode_str in line:\n",
    "                tetrode_str, tetrode_status = line.split(' ')\n",
    "                if int(tetrode_status) == 1:\n",
    "                    # then the tetrode is saved\n",
    "                    tetrode_str.find('_')\n",
    "                    tet_number = int(tetrode_str[tetrode_str.find('_') + 1:])\n",
    "                    active_tetrode.append(tet_number)\n",
    "\n",
    "    return active_tetrode\n",
    "\n",
    "\n",
    "def get_active_eeg(set_filename):\n",
    "    \"\"\"This will return a dictionary (cative_eeg_dict) where the keys\n",
    "    will be eeg channels from 1->64 which will represent the eeg suffixes (2 = .eeg2, 3 = 2.eeg3, etc)\n",
    "    and the key will be the channel that the EEG maps to (a channel from 0->63)\"\"\"\n",
    "    active_eeg = []\n",
    "    active_eeg_str = 'saveEEG_ch'\n",
    "\n",
    "    eeg_map = []\n",
    "    eeg_map_str = 'EEG_ch_'\n",
    "\n",
    "    active_eeg_dict = {}\n",
    "\n",
    "    with open(set_filename) as f:\n",
    "        for line in f:\n",
    "\n",
    "            if active_eeg_str in line:\n",
    "                # saveEEG_ch_X Y, where x is the eeg number, and Y is eitehr on or off (1 or 0)\n",
    "                _, status = line.split(' ')\n",
    "                active_eeg.append(int(status))\n",
    "            elif eeg_map_str in line:\n",
    "                # EEG_ch_X Y\n",
    "                _, chan = line.split(' ')\n",
    "                eeg_map.append(int(chan))\n",
    "\n",
    "                # active_eeg = np.asarray(active_eeg)\n",
    "                # eeg_map = np.asarray(eeg_map)\n",
    "\n",
    "    for i, status in enumerate(active_eeg):\n",
    "        if status == 1:\n",
    "            active_eeg_dict[i + 1] = eeg_map[i] - 1\n",
    "\n",
    "    return active_eeg_dict\n",
    "\n",
    "\n",
    "def is_egf_active(set_filename):\n",
    "    active_egf_str = 'saveEGF'\n",
    "\n",
    "    with open(set_filename) as f:\n",
    "        for line in f:\n",
    "\n",
    "            if active_egf_str in line:\n",
    "                _, egf_status = line.split(' ')\n",
    "\n",
    "                if int(egf_status) == 1:\n",
    "                    return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "def find_tetrodes(set_fullpath):\n",
    "    \"\"\"finds the tetrode files available for a given .set file if there is a  .cut file existing\"\"\"\n",
    "\n",
    "    tetrode_path, session = os.path.split(set_fullpath)\n",
    "    session, _ = os.path.splitext(session)\n",
    "\n",
    "    # getting all the files in that directory\n",
    "    file_list = os.listdir(tetrode_path)\n",
    "\n",
    "    # acquiring only a list of tetrodes that belong to that set file\n",
    "    tetrode_list = [os.path.join(tetrode_path, file) for file in file_list\n",
    "                    if is_tetrode(file, session)]\n",
    "\n",
    "    # if the .cut file doesn't exist remove list\n",
    "\n",
    "    tetrode_list = [file for file in tetrode_list if os.path.exists(\n",
    "        os.path.join(tetrode_path, '%s_%s.cut' % (os.path.splitext(file)[0], os.path.splitext(file)[1][1:])))]\n",
    "\n",
    "    return tetrode_list\n",
    "\n",
    "\n",
    "def find_unit(tetrode_list):\n",
    "    \"\"\"Inputs:\n",
    "    tetrode_list: list of tetrodes to find the units that are in the tetrode_path\n",
    "    example [r'C:Location\\of\\File\\filename.1', r'C:Location\\of\\File\\filename.2' ],\n",
    "    -------------------------------------------------------------\n",
    "    Outputs:\n",
    "    cut_list: an nx1 list for n-tetrodes in the tetrode_list containing a list of unit numbers that each spike belongs to\n",
    "    \"\"\"\n",
    "\n",
    "    input_list = True\n",
    "    if type(tetrode_list) != list:\n",
    "        input_list = False\n",
    "        tetrode_list = [tetrode_list]\n",
    "\n",
    "    cut_list = []\n",
    "    unique_cell_list = []\n",
    "    for tetrode_file in tetrode_list:\n",
    "        directory = os.path.dirname(tetrode_file)\n",
    "\n",
    "        try:\n",
    "            tetrode = int(os.path.splitext(tetrode_file)[1][1:])\n",
    "        except ValueError:\n",
    "            raise ValueError(\"The following file is invalid: %s\" % tetrode_file)\n",
    "\n",
    "        tetrode_base = os.path.splitext(os.path.basename(tetrode_file))[0]\n",
    "\n",
    "        cut_filename = os.path.join(directory, '%s_%d.cut' % (tetrode_base, tetrode))\n",
    "\n",
    "        cut_values = read_cut(cut_filename)\n",
    "        cut_list.append(cut_values)\n",
    "        unique_cell_list.append(np.unique(cut_values))\n",
    "\n",
    "    return cut_list\n",
    "\n",
    "\n",
    "def read_clu(filename):\n",
    "    \"\"\"\n",
    "    This will read in the .clu.N files that are provided by Tint. The .clu cell ID's go from 1 -> N\n",
    "    instead of the traditional 0->N-1 for a .cut file. We will convert from the 1->N format to the\n",
    "    0->N-1 format.\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.loadtxt(filename)\n",
    "\n",
    "    # the first number in the file is simply the number of cells that were recorded, we must remove this\n",
    "\n",
    "    # we will also subtract 1 to ensure that the data goes from 0->N-1 instead of 1->N. Essentially converting\n",
    "    # from the clu format to the .cut format.\n",
    "\n",
    "    return data[1:].flatten() - 1\n",
    "\n",
    "\n",
    "def read_cut(cut_filename):\n",
    "    \"\"\"This function will read the given cut file, and output the \"\"\"\n",
    "    cut_values = None\n",
    "    if os.path.exists(cut_filename):\n",
    "        extract_cut = False\n",
    "        with open(cut_filename, 'r') as f:\n",
    "            for line in f:\n",
    "                if 'Exact_cut' in line:  # finding the beginning of the cut values\n",
    "                    extract_cut = True\n",
    "                if extract_cut:  # read all the cut values\n",
    "                    cut_values = str(f.readlines())\n",
    "                    for string_val in ['\\\\n', ',', \"'\", '[', ']']:  # removing non base10 integer values\n",
    "                        cut_values = cut_values.replace(string_val, '')\n",
    "                    cut_values = [int(val) for val in cut_values.split()]\n",
    "        cut_values = np.asarray(cut_values)\n",
    "    return cut_values\n",
    "\n",
    "\n",
    "def getArenaParams(arena, conversion='', center=''):\n",
    "    \"\"\"\n",
    "    This is a legacy function back when we used to use the room name. Now I think it is best to just have the user\n",
    "    provide a center and pixel-per-meter value\n",
    "\n",
    "    :param arena:\n",
    "    :param conversion:\n",
    "    :param center:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if 'BehaviorRoom' in arena:\n",
    "        center = np.array([314.75, 390.5])\n",
    "        conversion = 495.5234\n",
    "    elif 'BehaviorRoom2' in arena:\n",
    "        # added december 12th, 2018\n",
    "        center = np.array([314.75, 390.5])\n",
    "        conversion = 485.1185\n",
    "    elif 'DarkRoom' in arena:\n",
    "        center = np.array([346.5, 273.5])\n",
    "        conversion = 711.3701\n",
    "    elif 'room4' in arena:\n",
    "        center = np.array([418, 186])\n",
    "        conversion = 313\n",
    "    elif arena in ['Linear Track', 'Circular Track', 'Four Leaf Clover Track', 'Simple Circular Track',\n",
    "                   'Parallel Linear Global Track', 'Parallel Linear Rate Track']:\n",
    "        center = center\n",
    "        conversion = conversion\n",
    "    else:\n",
    "        print(\"Room: \" + arena + \", is an unknown room!\")\n",
    "\n",
    "    return center, conversion\n",
    "\n",
    "\n",
    "def arena_config(posx, posy, ppm, center=None, flip_y=True):\n",
    "    \"\"\"\n",
    "    This function will convert the position values from the units of bits to centimeters. It will also center the arena\n",
    "    around the point (0,0) if you provide a center value. I have added a flip_y keyword argument because the Tint\n",
    "    program flips the y values so I will allow you to flip the values as well (or not).\n",
    "\n",
    "    :param posx:\n",
    "    :param posy:\n",
    "    :param arena:\n",
    "    :param conversion:\n",
    "    :param center:\n",
    "    :param flip_y: bool value that will determine if you want to flip y or not. When recording on Intan we inverted the\n",
    "    positions due to the camera position. However in the virtualmaze you might not want to flip y values.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if center is not None:\n",
    "        posx = 100 * (posx - center[0]) / ppm\n",
    "    else:\n",
    "        posx = 100 * (posx) / ppm\n",
    "\n",
    "    if flip_y:\n",
    "        # flip the y axis\n",
    "        if center is not None:\n",
    "            posy = 100 * (-posy + center[1]) / ppm\n",
    "        else:\n",
    "            posy = 100 * (-posy) / ppm\n",
    "    else:\n",
    "        if center is not None:\n",
    "            posy = 100 * (posy + center[1]) / ppm\n",
    "        else:\n",
    "            posy = 100 * (posy) / ppm\n",
    "\n",
    "    return posx, posy\n",
    "\n",
    "\n",
    "def ReadEEG(eeg_fname):\n",
    "    \"\"\"input:\n",
    "    eeg_filename: the fullpath to the eeg file that is desired to be read.\n",
    "    Example: C:\\Location\\of\\eegfile.eegX\n",
    "\n",
    "    Output:\n",
    "    The EEG waveform, and the sampling frequency\"\"\"\n",
    "\n",
    "    with open(eeg_fname, 'rb') as f:\n",
    "\n",
    "        is_eeg = False\n",
    "        if 'eeg' in eeg_fname:\n",
    "            is_eeg = True\n",
    "            # Fs = 250\n",
    "        # else:\n",
    "        #    Fs = 4.8e3\n",
    "\n",
    "        with contextlib.closing(mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)) as m:\n",
    "            # find the data_start\n",
    "            start_index = int(m.find(b'data_start') + len('data_start'))  # start of the data\n",
    "            stop_index = int(m.find(b'\\r\\ndata_end'))  # end of the data\n",
    "\n",
    "            sample_rate_start = m.find(b'sample_rate')\n",
    "            sample_rate_end = m[sample_rate_start:].find(b'\\r\\n')\n",
    "            Fs = float(m[sample_rate_start:sample_rate_start + sample_rate_end].decode('utf-8').split(' ')[1])\n",
    "\n",
    "            m = m[start_index:stop_index]\n",
    "\n",
    "            if is_eeg:\n",
    "                EEG = np.fromstring(m, dtype='>b')\n",
    "            else:\n",
    "                EEG = np.fromstring(m, dtype='<h')\n",
    "\n",
    "            return EEG, int(Fs)\n",
    "\n",
    "\n",
    "def EEG_to_Mat(input_filename, output_filename):\n",
    "    EEG, Fs = ReadEEG(input_filename)\n",
    "\n",
    "    if Fs > 250:\n",
    "        save_dictionary = {'EGF': EEG, 'Fs': Fs}\n",
    "    else:\n",
    "        save_dictionary = {'EEG': EEG, 'Fs': Fs}\n",
    "\n",
    "    savemat(output_filename, save_dictionary)\n",
    "\n",
    "\n",
    "def removeNan(posx, posy, post):\n",
    "    \"\"\"Remove any NaNs from the end of the array\"\"\"\n",
    "    removeNan = True\n",
    "    while removeNan:\n",
    "        if np.isnan(posx[-1]):\n",
    "            posx = posx[:-1]\n",
    "            posy = posy[:-1]\n",
    "            post = post[:-1]\n",
    "        else:\n",
    "            removeNan = False\n",
    "    return posx, posy, post\n",
    "\n",
    "\n",
    "def centerBox(posx, posy):\n",
    "    # must remove Nans first because the np.amin will return nan if there is a nan\n",
    "    posx = posx[~np.isnan(posx)]  # removes NaNs\n",
    "    posy = posy[~np.isnan(posy)]  # remove Nans\n",
    "\n",
    "    NE = np.array([np.amax(posx), np.amax(posy)])\n",
    "    NW = np.array([np.amin(posx), np.amax(posy)])\n",
    "    SW = np.array([np.amin(posx), np.amin(posy)])\n",
    "    SE = np.array([np.amax(posx), np.amin(posy)])\n",
    "\n",
    "    return findCenter(NE, NW, SW, SE)\n",
    "\n",
    "\n",
    "def findCenter(NE, NW, SW, SE):\n",
    "    \"\"\"Finds the center point (x,y) of the position boundaries\"\"\"\n",
    "\n",
    "    x = np.mean([np.amax([NE[0], SE[0]]), np.amin([NW[0], SW[0]])])\n",
    "    y = np.mean([np.amax([NW[1], NE[1]]), np.amin([SW[1], SE[1]])])\n",
    "    return np.array([x, y])\n",
    "\n",
    "\n",
    "def bits2uV(data, data_fpath, set_fpath=''):\n",
    "    '''\n",
    "\n",
    "    :param data:\n",
    "    :param data_fpath: example: 'C:\\example\\filepath.whatever'\n",
    "    :param set_fpath:\n",
    "    :return:\n",
    "    '''\n",
    "    path = os.path.split(data_fpath)[0]\n",
    "\n",
    "    if set_fpath == '':\n",
    "        set_fpath = os.path.join(path, ''.join([os.path.splitext(os.path.basename(data_fpath))[0],'.set']))\n",
    "\n",
    "    ext = os.path.splitext(data_fpath)[1]\n",
    "\n",
    "    if not os.path.exists(set_fpath):\n",
    "        error_message = 'The following setpath does not exist, cannot convert to uV: %s' % (set_fpath)\n",
    "        raise TintException(error_message)\n",
    "        #return error_message, 0\n",
    "\n",
    "    # create a tetrode map that has rows of channels that correspond to the same tetrode\n",
    "    tet_map = np.asarray([np.arange(start,start+4) for start in np.arange(0, 32)*4])\n",
    "\n",
    "    chan_gains = np.array([])\n",
    "    saved_eeg = np.array([])\n",
    "    eeg_chan_map = np.array([])\n",
    "\n",
    "    with open(set_fpath, 'r') as f:\n",
    "        for line in f:\n",
    "\n",
    "            if 'ADC_fullscale_mv' in line:\n",
    "                ADC_fullscale_mv = int(line.split(\" \")[1])\n",
    "            elif 'gain_ch_' in line:\n",
    "                # create an array of channel gains [channel_number, channels_gain]\n",
    "                if len(chan_gains) == 0:\n",
    "                    chan_gains = np.array([int(line[len('gain_ch_'):line.find(\" \")]), int(line.split(\" \")[1])], ndmin=2)\n",
    "                else:\n",
    "                    chan_gains = np.append(chan_gains, np.array([int(line[len('gain_ch_'):line.find(\" \")]), int(line.split(\" \")[1])], ndmin=2), axis=0)\n",
    "            elif 'saveEEG_ch_' in line:\n",
    "                # create an array of EEG channels that are saved\n",
    "                if int(line.split(\" \")[1]) == 1:\n",
    "                    if len(chan_gains) == 0:\n",
    "                        saved_eeg = np.array([int(line[len('saveEEG_ch_'):line.find(\" \")])])\n",
    "                    else:\n",
    "                        saved_eeg = np.append(saved_eeg, np.array([int(line[len('saveEEG_ch_'):line.find(\" \")])]))\n",
    "            elif 'EEG_ch_' in line and 'BPF' not in line:\n",
    "                if len(eeg_chan_map) == 0:\n",
    "                    eeg_chan_map = np.array([int(line[len('EEG_ch_'):line.find(\" \")]), int(line.split(\" \")[1])], ndmin=2)\n",
    "                else:\n",
    "                    eeg_chan_map = np.append(eeg_chan_map, np.array([int(line[len('EEG_ch_'):line.find(\" \")]), int(line.split(\" \")[1])], ndmin=2), axis=0)\n",
    "\n",
    "    if '.eeg' in ext:\n",
    "        if len(ext) == len('.eeg'):\n",
    "            chan_num = 1\n",
    "        else:\n",
    "            chan_num = int(ext[len('.eeg'):])\n",
    "\n",
    "        for index, value in enumerate(eeg_chan_map[:]):\n",
    "            if value[0] == chan_num:\n",
    "                eeg_chan = value[1] - 1\n",
    "                break\n",
    "\n",
    "        for index, value in enumerate(chan_gains):\n",
    "            if value[0] == eeg_chan:\n",
    "                gain = value[1]\n",
    "\n",
    "        scalar = ADC_fullscale_mv*1000/(gain*128)\n",
    "        if len(data) == 0:\n",
    "            data_uV = []\n",
    "        else:\n",
    "            data_uV = np.multiply(data, scalar)\n",
    "            #print(data_uV)\n",
    "\n",
    "    elif '.egf' in ext:\n",
    "        if len(ext) == len('.egf'):\n",
    "            chan_num = 1\n",
    "        else:\n",
    "            chan_num = int(ext[len('.egf'):])\n",
    "\n",
    "        for index, value in enumerate(eeg_chan_map[:]):\n",
    "            if value[0] == chan_num:\n",
    "                eeg_chan = value[1] - 1\n",
    "                break\n",
    "\n",
    "        for index, value in enumerate(chan_gains):\n",
    "            if value[0] == eeg_chan:\n",
    "                gain = value[1]\n",
    "                break\n",
    "\n",
    "        scalar = ADC_fullscale_mv*1000/(gain*32768)\n",
    "\n",
    "        if len(data) == 0:\n",
    "            data_uV = []\n",
    "        else:\n",
    "            data_uV = np.multiply(data, scalar)\n",
    "\n",
    "    else:\n",
    "        tetrode_num = int(ext[1:])\n",
    "\n",
    "        tet_chans = tet_map[tetrode_num-1]\n",
    "\n",
    "        gain = np.asarray([[gains[1] for gains in chan_gains if gains[0] == chan] for chan in tet_chans])\n",
    "\n",
    "        scalar = (ADC_fullscale_mv*1000/(gain*128).reshape((1, len(gain))))[0]\n",
    "\n",
    "        if len(data) == 0:\n",
    "            data_uV = []\n",
    "        else:\n",
    "            data_uV = np.multiply(data, scalar)\n",
    "\n",
    "    return data_uV, scalar\n",
    "\n",
    "\n",
    "def getspikes(fullpath):\n",
    "    \"\"\"\n",
    "    This function will return the spike data, spike times, and spike parameters from Tint tetrode data.\n",
    "\n",
    "    Example:\n",
    "        tetrode_fullpath = 'C:\\\\example\\\\tetrode_1.1'\n",
    "        ts, ch1, ch2, ch3, ch4, spikeparam = getspikes(tetrode_fullpath)\n",
    "\n",
    "    Args:\n",
    "        fullpath (str): the fullpath to the Tint tetrode file you want to acquire the spike data from.\n",
    "\n",
    "    Returns:\n",
    "        ts (ndarray): an Nx1 array for the spike times, where N is the number of spikes.\n",
    "        ch1 (ndarray) an NxM matrix containing the spike data for channel 1, N is the number of spikes,\n",
    "            and M is the chunk length.\n",
    "        ch2 (ndarray) an NxM matrix containing the spike data for channel 2, N is the number of spikes,\n",
    "            and M is the chunk length.\n",
    "        ch3 (ndarray) an NxM matrix containing the spike data for channel 3, N is the number of spikes,\n",
    "            and M is the chunk length.\n",
    "        ch4 (ndarray) an NxM matrix containing the spike data for channel 4, N is the number of spikes,\n",
    "            and M is the chunk length.\n",
    "        spikeparam (dict): a dictionary containing the header values from the tetrode file.\n",
    "    \"\"\"\n",
    "    spikes, spikeparam = importspikes(fullpath)\n",
    "    ts = spikes['t']\n",
    "    nspk = spikeparam['num_spikes']\n",
    "    spikelen = spikeparam['samples_per_spike']\n",
    "\n",
    "    ch1 = spikes['ch1']\n",
    "    ch2 = spikes['ch2']\n",
    "    ch3 = spikes['ch3']\n",
    "    ch4 = spikes['ch4']\n",
    "\n",
    "    return ts, ch1, ch2, ch3, ch4, spikeparam\n",
    "\n",
    "\n",
    "def importspikes(filename):\n",
    "    \"\"\"Reads through the tetrode file as an input and returns two things, a dictionary containing the following:\n",
    "    timestamps, ch1-ch4 waveforms, and it also returns a dictionary containing the spike parameters\"\"\"\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        for line in f:\n",
    "            if 'data_start' in str(line):\n",
    "                spike_data = np.fromstring((line + f.read())[len('data_start'):-len('\\r\\ndata_end\\r\\n')], dtype='uint8')\n",
    "                break\n",
    "            elif 'num_spikes' in str(line):\n",
    "                num_spikes = int(line.decode(encoding='UTF-8').split(\" \")[1])\n",
    "            elif 'bytes_per_timestamp' in str(line):\n",
    "                bytes_per_timestamp = int(line.decode(encoding='UTF-8').split(\" \")[1])\n",
    "            elif 'samples_per_spike' in str(line):\n",
    "                samples_per_spike = int(line.decode(encoding='UTF-8').split(\" \")[1])\n",
    "            elif 'bytes_per_sample' in str(line):\n",
    "                bytes_per_sample = int(line.decode(encoding='UTF-8').split(\" \")[1])\n",
    "            elif 'timebase' in str(line):\n",
    "                timebase = int(line.decode(encoding='UTF-8').split(\" \")[1])\n",
    "            elif 'duration' in str(line):\n",
    "                duration = int(line.decode(encoding='UTF-8').split(\" \")[1])\n",
    "            # elif 'sample_rate' in str(line):\n",
    "            #     samp_rate = int(line.decode(encoding='UTF-8').split(\" \")[1])\n",
    "\n",
    "    samp_rate = get_spike_sample_rate(filename)\n",
    "\n",
    "    # calculating the big-endian and little endian matrices so we can convert from bytes -> decimal\n",
    "    big_endian_vector = 256 ** np.arange(bytes_per_timestamp - 1, -1, -1)\n",
    "    little_endian_matrix = np.arange(0, bytes_per_sample).reshape(bytes_per_sample, 1)\n",
    "    little_endian_matrix = 256 ** numpy.matlib.repmat(little_endian_matrix, 1, samples_per_spike)\n",
    "\n",
    "    number_channels = 4\n",
    "\n",
    "    # calculating the timestamps\n",
    "    t_start_indices = np.linspace(0, num_spikes * (bytes_per_sample * samples_per_spike * 4 +\n",
    "                                                   bytes_per_timestamp * 4), num=num_spikes, endpoint=False).astype(\n",
    "        int).reshape(num_spikes, 1)\n",
    "    t_indices = t_start_indices\n",
    "\n",
    "    for chan in np.arange(1, number_channels):\n",
    "        t_indices = np.hstack((t_indices, t_start_indices + chan))\n",
    "\n",
    "    t = spike_data[t_indices].reshape(num_spikes, bytes_per_timestamp)  # acquiring the time bytes\n",
    "    t = np.sum(np.multiply(t, big_endian_vector), axis=1) / timebase  # converting from bytes to float values\n",
    "    t_indices = None\n",
    "\n",
    "    waveform_data = np.zeros((number_channels, num_spikes, samples_per_spike))  # (dimensions, rows, columns)\n",
    "\n",
    "    bytes_offset = 0\n",
    "    # read the t,ch1,t,ch2,t,ch3,t,ch4\n",
    "\n",
    "    for chan in range(number_channels):  # only really care about the first time that gets written\n",
    "        chan_start_indices = t_start_indices + chan * samples_per_spike + bytes_per_timestamp + bytes_per_timestamp * chan\n",
    "        for spike_sample in np.arange(1, samples_per_spike):\n",
    "            chan_start_indices = np.hstack((chan_start_indices, t_start_indices +\n",
    "                                            chan * samples_per_spike + bytes_per_timestamp +\n",
    "                                            bytes_per_timestamp * chan + spike_sample))\n",
    "        waveform_data[chan][:][:] = spike_data[chan_start_indices].reshape(num_spikes, samples_per_spike).astype(\n",
    "            'int8')  # acquiring the channel bytes\n",
    "        waveform_data[chan][:][:][np.where(waveform_data[chan][:][:] > 127)] -= 256\n",
    "        waveform_data[chan][:][:] = np.multiply(waveform_data[chan][:][:], little_endian_matrix)\n",
    "\n",
    "    spikeparam = {'timebase': timebase, 'bytes_per_sample': bytes_per_sample, 'samples_per_spike': samples_per_spike,\n",
    "                  'bytes_per_timestamp': bytes_per_timestamp, 'duration': duration, 'num_spikes': num_spikes,\n",
    "                  'sample_rate': samp_rate}\n",
    "\n",
    "    return {'t': t.reshape(num_spikes, 1), 'ch1': np.asarray(waveform_data[0][:][:]),\n",
    "            'ch2': np.asarray(waveform_data[1][:][:]),\n",
    "            'ch3': np.asarray(waveform_data[2][:][:]), 'ch4': np.asarray(waveform_data[3][:][:])}, spikeparam\n",
    "\n",
    "\n",
    "def speed2D(x, y, t):\n",
    "    \"\"\"calculates an averaged/smoothed speed\"\"\"\n",
    "\n",
    "    N = len(x)\n",
    "    v = np.zeros((N, 1))\n",
    "\n",
    "    for index in range(1, N-1):\n",
    "        v[index] = np.sqrt((x[index + 1] - x[index - 1]) ** 2 + (y[index + 1] - y[index - 1]) ** 2) / (\n",
    "        t[index + 1] - t[index - 1])\n",
    "\n",
    "    v[0] = v[1]\n",
    "    v[-1] = v[-2]\n",
    "\n",
    "    return v\n",
    "\n",
    "\n",
    "def fixTimestamps(post):\n",
    "    first = post[0]\n",
    "    N = len(post)\n",
    "    uniquePost = np.unique(post)\n",
    "\n",
    "    if len(uniquePost) != N:\n",
    "        didFix = True\n",
    "        numZeros = 0\n",
    "        # find the number of zeros at the end of the file\n",
    "\n",
    "        while True:\n",
    "            if post[-1 - numZeros] == 0:\n",
    "                numZeros += 1\n",
    "            else:\n",
    "                break\n",
    "        last = first + (N-1-numZeros)*0.02\n",
    "        fixedPost = np.arange(first, last+0.02, 0.02)\n",
    "        fixedPost = fixedPost.reshape((len(fixedPost), 1))\n",
    "\n",
    "    else:\n",
    "        didFix = False\n",
    "        fixedPost = []\n",
    "\n",
    "    return didFix, fixedPost\n",
    "\n",
    "\n",
    "def remBadTrack(x, y, t, threshold):\n",
    "    \"\"\"function [x,y,t] = remBadTrack(x,y,t,treshold)\n",
    "\n",
    "    % Indexes to position samples that are to be removed\n",
    "   \"\"\"\n",
    "\n",
    "    remInd = []\n",
    "    diffx = np.diff(x, axis=0)\n",
    "    diffy = np.diff(y, axis=0)\n",
    "    diffR = np.sqrt(diffx ** 2 + diffy ** 2)\n",
    "\n",
    "    # the MATLAB works fine without NaNs, if there are Nan's just set them to threshold they will be removed later\n",
    "    diffR[np.isnan(diffR)] = threshold # setting the nan values to threshold\n",
    "    ind = np.where((diffR > threshold))[0]\n",
    "\n",
    "    if len(ind) == 0:  # no bad samples to remove\n",
    "        return x, y, t\n",
    "\n",
    "    if ind[-1] == len(x):\n",
    "        offset = 2\n",
    "    else:\n",
    "        offset = 1\n",
    "\n",
    "    for index in range(len(ind) - offset):\n",
    "        if ind[index + 1] == ind[index] + 1:\n",
    "            # A single sample position jump, tracker jumps out one sample and\n",
    "            # then jumps back to path on the next sample. Remove bad sample.\n",
    "            remInd.append(ind[index] + 1)\n",
    "        else:\n",
    "            ''' Not a single jump. 2 possibilities:\n",
    "             1. Tracker jumps out, and stay out at the same place for several\n",
    "             samples and then jumps back.\n",
    "             2. Tracker just has a small jump before path continues as normal,\n",
    "             unknown reason for this. In latter case the samples are left\n",
    "             untouched'''\n",
    "            idx = np.where(x[ind[index] + 1:ind[index + 1] + 1 + 1] == x[ind[index] + 1])[0]\n",
    "            if len(idx) == len(x[ind[index] + 1:ind[index + 1] + 1 + 1]):\n",
    "                remInd.extend(\n",
    "                    list(range(ind[index] + 1, ind[index + 1] + 1 + 1)))  # have that extra since range goes to end-1\n",
    "    # keep_ind = [val for val in range(len(x)) if val not in remInd]\n",
    "    keep_ind = np.setdiff1d(np.arange(len(x)), remInd)\n",
    "\n",
    "    # avoid trying to slice with an invalid index\n",
    "    if keep_ind[-1] == len(x):\n",
    "        keep_ind = keep_ind[:-1]\n",
    "\n",
    "    x = x[keep_ind]\n",
    "    y = y[keep_ind]\n",
    "    t = t[keep_ind]\n",
    "\n",
    "    return x.reshape((len(x), 1)), y.reshape((len(y), 1)), t.reshape((len(t), 1))\n",
    "\n",
    "\n",
    "def visitedBins(x, y, mapAxis):\n",
    "\n",
    "    binWidth = mapAxis[1]-mapAxis[0]\n",
    "\n",
    "    N = len(mapAxis)\n",
    "    visited = np.zeros((N, N))\n",
    "\n",
    "    for col in range(N):\n",
    "        for row in range(N):\n",
    "            px = mapAxis[col]\n",
    "            py = mapAxis[row]\n",
    "            distance = np.sqrt((px-x)**2 + (py-y)**2)\n",
    "\n",
    "            if np.amin(distance) <= binWidth:\n",
    "                visited[row, col] = 1\n",
    "\n",
    "    return visited\n",
    "\n",
    "\n",
    "def spikePos(ts, x, y, t, cPost, shuffleSpks, shuffleCounter=True):\n",
    "\n",
    "    randtime = 0\n",
    "\n",
    "    if shuffleSpks:\n",
    "\n",
    "        # create a random sample to shuffle from -20 to 20 (not including 0)\n",
    "        randsamples = np.asarray([sample_num for sample_num in range(-20, 21) if sample_num != 0])\n",
    "\n",
    "        if shuffleCounter:\n",
    "            randtime = 0\n",
    "        else:\n",
    "            randtime = np.random.choice(randsamples, replace=False)\n",
    "\n",
    "            maxts = max(ts)\n",
    "            ts += randtime\n",
    "\n",
    "            if np.sign(randtime) < 0:\n",
    "                for index in range(len(ts)):\n",
    "                    if ts[index] < 0:\n",
    "                        ts[index] = maxts + np.absolute(randtime) + ts[index]\n",
    "\n",
    "            elif np.sign(randtime) > 0:\n",
    "                for index in range(len(ts)):\n",
    "                    if ts[index] > maxts:\n",
    "                        ts[index] = ts[index] - maxts\n",
    "\n",
    "            ts = np.sort(ts)\n",
    "    else:\n",
    "        ts = np.roll(ts, randtime)\n",
    "\n",
    "    N = len(ts)\n",
    "    spkx = np.zeros((N, 1))\n",
    "    spky = np.zeros_like(spkx)\n",
    "    newTs = np.zeros_like(spkx)\n",
    "    count = -1 # need to subtract 1 because the python indices start at 0 and MATLABs at 1\n",
    "\n",
    "    for index in range(N):\n",
    "        tdiff = (t - ts[index])**2\n",
    "        tdiff2 = (cPost-ts[index])**2\n",
    "        m = np.amin(tdiff)\n",
    "        ind = np.where(tdiff == m)[0]\n",
    "\n",
    "        m2 = np.amin(tdiff2)\n",
    "        #ind2 = np.where(tdiff2 == m2)[0]\n",
    "\n",
    "        if m == m2:\n",
    "            count += 1\n",
    "            spkx[count] = x[ind[0]]\n",
    "            spky[count] = y[ind[0]]\n",
    "            newTs[count] = ts[index]\n",
    "\n",
    "    spkx = spkx[:count + 1]\n",
    "    spky = spky[:count + 1]\n",
    "    newTs = newTs[:count + 1]\n",
    "\n",
    "    return spkx, spky, newTs, randtime\n",
    "\n",
    "\n",
    "def ratemap(spike_x, spike_y, posx, posy, post, h, yAxis, xAxis):\n",
    "    invh = 1/h\n",
    "    map = np.zeros((len(xAxis), len(yAxis)))\n",
    "    pospdf = np.zeros_like(map)\n",
    "\n",
    "    current_Y = -1\n",
    "    for Y in yAxis:\n",
    "        current_Y +=1\n",
    "        current_X = -1\n",
    "        for X in xAxis:\n",
    "            current_X += 1\n",
    "            map[current_Y, current_X], pospdf[current_Y, current_X] = rate_estimator(spike_x, spike_y, X, Y, invh, posx, posy, post)\n",
    "    pospdf = pospdf / np.sum(np.sum(pospdf))\n",
    "    return map, pospdf\n",
    "\n",
    "\n",
    "def rate_estimator(spike_x, spike_y, x, y, invh, posx, posy, post):\n",
    "    '''Calculate the rate for one position value.\n",
    "    edge-corrected kernel density estimator'''\n",
    "    conv_sum = np.sum(gaussian_kernel((spike_x-x)*invh, (spike_y-y)*invh))\n",
    "    edge_corrector = np.trapz(gaussian_kernel(((posx-x)*invh),((posy-y)*invh)), post, axis=0)\n",
    "    r = (conv_sum / (edge_corrector + 0.0001)) + 0.0001 # regularised firing rate for \"wellbehavedness\" i.e. no division by zero or log of zero\n",
    "    return r, edge_corrector\n",
    "\n",
    "\n",
    "def gaussian_kernel(x, y):\n",
    "    '''Gaussian kernel for the rate calculation:\n",
    "    % k(u) = ((2*pi)^(-length(u)/2)) * exp(u'*u)'''\n",
    "    r = 0.15915494309190 * np.exp(-0.5 * (np.multiply(x,x) + np.multiply(y,y)));\n",
    "    return r\n",
    "\n",
    "\n",
    "def detect_peaks(x, mph=None, mpd=1, threshold=0, edge='rising',\n",
    "                 kpsh=False, valley=False, show=False, ax=None):\n",
    "    __author__ = \"Marcos Duarte, https://github.com/demotu/BMC\"\n",
    "    __version__ = \"1.0.4\"\n",
    "    __license__ = \"MIT\"\n",
    "\n",
    "    \"\"\"Detect peaks in data based on their amplitude and other features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1D array_like\n",
    "        data.\n",
    "    mph : {None, number}, optional (default = None)\n",
    "        detect peaks that are greater than minimum peak height.\n",
    "    mpd : positive integer, optional (default = 1)\n",
    "        detect peaks that are at least separated by minimum peak distance (in\n",
    "        number of data).\n",
    "    threshold : positive number, optional (default = 0)\n",
    "        detect peaks (valleys) that are greater (smaller) than `threshold`\n",
    "        in relation to their immediate neighbors.\n",
    "    edge : {None, 'rising', 'falling', 'both'}, optional (default = 'rising')\n",
    "        for a flat peak, keep only the rising edge ('rising'), only the\n",
    "        falling edge ('falling'), both edges ('both'), or don't detect a\n",
    "        flat peak (None).\n",
    "    kpsh : bool, optional (default = False)\n",
    "        keep peaks with same height even if they are closer than `mpd`.\n",
    "    valley : bool, optional (default = False)\n",
    "        if True (1), detect valleys (local minima) instead of peaks.\n",
    "    show : bool, optional (default = False)\n",
    "        if True (1), plot data in matplotlib figure.\n",
    "    ax : a matplotlib.axes.Axes instance, optional (default = None).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ind : 1D array_like\n",
    "        indeces of the peaks in `x`.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The detection of valleys instead of peaks is performed internally by simply\n",
    "    negating the data: `ind_valleys = detect_peaks(-x)`\n",
    "\n",
    "    The function can handle NaN's\n",
    "\n",
    "    See this IPython Notebook [1]_.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] http://nbviewer.ipython.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from detect_peaks import detect_peaks\n",
    "    >>> x = np.random.randn(100)\n",
    "    >>> x[60:81] = np.nan\n",
    "    >>> # detect all peaks and plot data\n",
    "    >>> ind = detect_peaks(x, show=True)\n",
    "    >>> print(ind)\n",
    "\n",
    "    >>> x = np.sin(2*np.pi*5*np.linspace(0, 1, 200)) + np.random.randn(200)/5\n",
    "    >>> # set minimum peak height = 0 and minimum peak distance = 20\n",
    "    >>> detect_peaks(x, mph=0, mpd=20, show=True)\n",
    "\n",
    "    >>> x = [0, 1, 0, 2, 0, 3, 0, 2, 0, 1, 0]\n",
    "    >>> # set minimum peak distance = 2\n",
    "    >>> detect_peaks(x, mpd=2, show=True)\n",
    "\n",
    "    >>> x = np.sin(2*np.pi*5*np.linspace(0, 1, 200)) + np.random.randn(200)/5\n",
    "    >>> # detection of valleys instead of peaks\n",
    "    >>> detect_peaks(x, mph=0, mpd=20, valley=True, show=True)\n",
    "\n",
    "    >>> x = [0, 1, 1, 0, 1, 1, 0]\n",
    "    >>> # detect both edges\n",
    "    >>> detect_peaks(x, edge='both', show=True)\n",
    "\n",
    "    >>> x = [-2, 1, -2, 2, 1, 1, 3, 0]\n",
    "    >>> # set threshold = 2\n",
    "    >>> detect_peaks(x, threshold = 2, show=True)\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.atleast_1d(x).astype('float64')\n",
    "    if x.size < 3:\n",
    "        return np.array([], dtype=int)\n",
    "    if valley:\n",
    "        x = -x\n",
    "    # find indices of all peaks\n",
    "    dx = x[1:] - x[:-1]\n",
    "    # handle NaN's\n",
    "    indnan = np.where(np.isnan(x))[0]\n",
    "    if indnan.size:\n",
    "        x[indnan] = np.inf\n",
    "        dx[np.where(np.isnan(dx))[0]] = np.inf\n",
    "    ine, ire, ife = np.array([[], [], []], dtype=int)\n",
    "    if not edge:\n",
    "        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "    else:\n",
    "        if edge.lower() in ['rising', 'both']:\n",
    "            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "        if edge.lower() in ['falling', 'both']:\n",
    "            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]\n",
    "    ind = np.unique(np.hstack((ine, ire, ife)))\n",
    "    # handle NaN's\n",
    "    if ind.size and indnan.size:\n",
    "        # NaN's and values close to NaN's cannot be peaks\n",
    "        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan - 1, indnan + 1))), invert=True)]\n",
    "    # first and last values of x cannot be peaks\n",
    "    if ind.size and ind[0] == 0:\n",
    "        ind = ind[1:]\n",
    "    if ind.size and ind[-1] == x.size - 1:\n",
    "        ind = ind[:-1]\n",
    "    # remove peaks < minimum peak height\n",
    "    if ind.size and mph is not None:\n",
    "        ind = ind[x[ind] >= mph]\n",
    "    # remove peaks - neighbors < threshold\n",
    "    if ind.size and threshold > 0:\n",
    "        dx = np.min(np.vstack([x[ind] - x[ind - 1], x[ind] - x[ind + 1]]), axis=0)\n",
    "        ind = np.delete(ind, np.where(dx < threshold)[0])\n",
    "    # detect small peaks closer than minimum peak distance\n",
    "    if ind.size and mpd > 1:\n",
    "        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height\n",
    "        idel = np.zeros(ind.size, dtype=bool)\n",
    "        for i in range(ind.size):\n",
    "            if not idel[i]:\n",
    "                # keep peaks with the same height if kpsh is True\n",
    "                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \\\n",
    "                              & (x[ind[i]] > x[ind] if kpsh else True)\n",
    "                idel[i] = 0  # Keep current peak\n",
    "        # remove the small peaks and sort back the indices by their occurrence\n",
    "        ind = np.sort(ind[~idel])\n",
    "\n",
    "    if show:\n",
    "        if indnan.size:\n",
    "            x[indnan] = np.nan\n",
    "        if valley:\n",
    "            x = -x\n",
    "        _plot(x, mph, mpd, threshold, edge, valley, ax, ind)\n",
    "\n",
    "    return ind\n",
    "\n",
    "\n",
    "def _plot(x, mph, mpd, threshold, edge, valley, ax, ind):\n",
    "    \"\"\"Plot results of the detect_peaks function, see its help.\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError:\n",
    "        print('matplotlib is not available.')\n",
    "    else:\n",
    "        if ax is None:\n",
    "            _, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "        ax.plot(x, 'b', lw=1)\n",
    "        if ind.size:\n",
    "            label = 'valley' if valley else 'peak'\n",
    "            label = label + 's' if ind.size > 1 else label\n",
    "            ax.plot(ind, x[ind], '+', mfc=None, mec='r', mew=2, ms=8,\n",
    "                    label='%d %s' % (ind.size, label))\n",
    "            ax.legend(loc='best', framealpha=.5, numpoints=1)\n",
    "        ax.set_xlim(-.02 * x.size, x.size * 1.02 - 1)\n",
    "        ymin, ymax = x[np.isfinite(x)].min(), x[np.isfinite(x)].max()\n",
    "        yrange = ymax - ymin if ymax > ymin else 1\n",
    "        ax.set_ylim(ymin - 0.1 * yrange, ymax + 0.1 * yrange)\n",
    "        ax.set_xlabel('Data #', fontsize=14)\n",
    "        ax.set_ylabel('Amplitude', fontsize=14)\n",
    "        mode = 'Valley detection' if valley else 'Peak detection'\n",
    "        ax.set_title(\"%s (mph=%s, mpd=%d, threshold=%s, edge='%s')\"\n",
    "                     % (mode, str(mph), mpd, str(threshold), edge))\n",
    "        # plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def get_spike_color(cell_number):\n",
    "\n",
    "    \"\"\"This method will match the cell number with the color it should be RGB in Tint.\n",
    "\n",
    "    These cells are numbered from 1-30 (there is technically a zeroth cell, but that isn't plotted\"\"\"\n",
    "    spike_colors = [(1, 8, 184), (93, 249, 75), (234, 8, 9),\n",
    "                         (229, 22, 239), (80, 205, 243), (27, 164, 0),\n",
    "                         (251, 188, 56), (27, 143, 167), (127, 41, 116),\n",
    "                         (191, 148, 23), (185, 9, 17), (231, 223, 67),\n",
    "                         (144, 132, 145), (34, 236, 228), (217, 20, 145),\n",
    "                         (172, 64, 80), (176, 106, 138), (199, 194, 167),\n",
    "                         (216, 204, 105), (160, 204, 61), (187, 81, 88),\n",
    "                         (45, 216, 122), (242, 136, 25), (50, 164, 161),\n",
    "                         (249, 67, 16), (252, 232, 147), (114, 156, 238),\n",
    "                         (241, 212, 179), (129, 62, 162), (235, 133, 126)]\n",
    "\n",
    "    return spike_colors[int(cell_number)-1]\n",
    "\n",
    "\n",
    "def get_spike_sample_rate(session_basename):\n",
    "    \"\"\"\n",
    "    This function will return the sampling frequency that the spike data was recorded in.\n",
    "\n",
    "    Although the rawRate stated in the .set filename will say 48k, it could also be 24k.\n",
    "    This will check if the signal was actually recorded in 24k (looking at the Spike2msMode\n",
    "    value in the .set file)\n",
    "\n",
    "    \"\"\"\n",
    "    # just in case a file was provided with an extension, this will just get the basename\n",
    "    basename = os.path.splitext(session_basename)[0]\n",
    "\n",
    "    # get the .set filename so that we can determine the sample rate\n",
    "    set_filename = '%s.set' % basename\n",
    "\n",
    "    if not os.path.exists(set_filename):\n",
    "        raise ValueError(\"The following .set filename does not exist: %s!\" % set_filename)\n",
    "\n",
    "    sample_rate = float(get_setfile_parameter('rawRate', set_filename))\n",
    "\n",
    "    # however, when recording in the 48k mode, it will say the rawRate is 48000, even though it should be 24000\n",
    "\n",
    "    # check if the 24k mode is active (2ms long signals)\n",
    "    two_ms_mode = int(get_setfile_parameter('Spike2msMode', set_filename))\n",
    "\n",
    "    if two_ms_mode == 1:\n",
    "        # then the 24k mode is active, return 24k\n",
    "        return 24e3\n",
    "    else:\n",
    "        # the 24k mode is not active, return the sample_rate\n",
    "        return sample_rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('envPRISM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "271de3eaf5512a01a3a2cea9253de8f7a978ec97e5a00bc2131d971ee349090f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
