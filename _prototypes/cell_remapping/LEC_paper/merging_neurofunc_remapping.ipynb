{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to make edits to repo without having to restart notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, mannwhitneyu, wilcoxon, ttest_rel, ttest_ind\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from matplotlib.colors import ColorConverter\n",
    "\n",
    "remap_path = os.getcwd()\n",
    "prototype_path = os.path.abspath(os.path.join(remap_path, os.pardir))\n",
    "project_path = os.path.abspath(os.path.join(prototype_path, os.pardir))\n",
    "lab_path = os.path.abspath(os.path.join(project_path, os.pardir))\n",
    "sys.path.append(project_path)\n",
    "os.chdir(project_path)\n",
    "sys.path.append(project_path)\n",
    "print(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.batch_map.LEC_naming import LEC_naming_format, extract_name_lec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANT_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_remapping\\ANT\"\n",
    "NON_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_remapping\\NON\"\n",
    "B6_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_remapping\\B6\"\n",
    "dfs = []\n",
    "for path in [ANT_path, NON_path, B6_path]:\n",
    "    for animal_dir in os.listdir(path):\n",
    "        for csv_file in os.listdir(path + r'\\\\' + animal_dir):\n",
    "            if csv_file.endswith('.xlsx'):\n",
    "                file_df = pd.read_excel(path + r'\\\\' + animal_dir + r'\\\\' + csv_file)\n",
    "                # add column for genotype\n",
    "                if path == ANT_path:\n",
    "                    file_df['group'] = 'ANT'\n",
    "                elif path == NON_path:\n",
    "                    file_df['group'] = 'NON'\n",
    "                elif path == B6_path:\n",
    "                    file_df['group'] = 'B6'\n",
    "                dfs.append(file_df)\n",
    "\n",
    "df_remapping = pd.concat(dfs, ignore_index=True)\n",
    "df_remapping.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match remapping csvs to neurofunc csvss\n",
    "\n",
    "ANT_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_neurofunc\\ANT\"\n",
    "NON_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_neurofunc\\NON\"\n",
    "B6_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_neurofunc\\B6\"\n",
    "dfs = []\n",
    "for path in [ANT_path, NON_path, B6_path]:\n",
    "    for csv_file in os.listdir(path):\n",
    "        if csv_file.endswith('.xlsx') and '~$' not in csv_file:\n",
    "            print(csv_file)\n",
    "            file_df = pd.read_excel(path + r'\\\\' + csv_file)\n",
    "            name = csv_file.split('_')[0]\n",
    "            for i, row in file_df.iterrows():\n",
    "                signature = row['Session']\n",
    "                # print(name)\n",
    "                if name not in signature:\n",
    "                    signature = name + '_' + signature\n",
    "                    file_df.at[i, 'Session'] = signature\n",
    "\n",
    "            # add column for genotype\n",
    "            if path == ANT_path:\n",
    "                file_df['group'] = 'ANT'\n",
    "            elif path == NON_path:\n",
    "                file_df['group'] = 'NON'\n",
    "            elif path == B6_path:\n",
    "                file_df['group'] = 'B6'\n",
    "            dfs.append(file_df)\n",
    "\n",
    "df_neurofunc = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "NON_set_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_set_files\\NON\"\n",
    "ANT_set_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_set_files\\ANT\"\n",
    "B6_set_path = r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\LEC_set_files\\B6\"\n",
    "# insert column for date_time into df_neurofunc\n",
    "df_neurofunc['date_time'] = pd.NaT\n",
    "df_neurofunc['ses_ct'] = pd.NaT\n",
    "\n",
    "for path in [ANT_set_path, NON_set_path, B6_set_path]:\n",
    "    for animal_dir in os.listdir(path):\n",
    "        for set_dir in os.listdir(path + r'\\\\' + animal_dir):\n",
    "            date_times = {}\n",
    "            # sigs = []\n",
    "            for set_file in os.listdir(path + r'\\\\' + animal_dir + r'\\\\' + set_dir):\n",
    "                # print(set_file)\n",
    "                assert set_file.endswith('.set')\n",
    "                ses_signature = set_file.split('.')[0]\n",
    "\n",
    "                # sigs.append(ses_signature)\n",
    "                # print(set_file)\n",
    "                with open(path + r'\\\\' + animal_dir + r'\\\\' + set_dir + r'\\\\' + set_file, 'r') as f:\n",
    "                    for line in f:\n",
    "                        if 'trial_time' in str(line):\n",
    "                            # trial_time = line.decode(encoding='UTF-8').split(\" \")[1]\n",
    "                            trial_time = line.split(\" \")[1]\n",
    "                        if 'trial_date' in str(line):\n",
    "                            trial_date = line.split(\" \")[2:]\n",
    "\n",
    "                    day, month, year = trial_date\n",
    "                    month = datetime.strptime(str(month), '%b').month\n",
    "                    hour, minute, second = trial_time.split(':')\n",
    "                    date_time = datetime(int(year), int(month), int(day), int(hour), int(minute), int(second))\n",
    "\n",
    "                # assert ses_signature not in date_times.keys(), 'Session signature {} already in dictionary'.format(ses_signature)\n",
    "                # assert date_time not in date_times.values(), 'Date time {} already in dictionary'.format(date_time) \n",
    "                date_times[ses_signature] = date_time\n",
    "\n",
    "                print(date_time, ses_signature)\n",
    "\n",
    "            # sort dictionary by date_time\n",
    "            sigs = list(date_times.keys())\n",
    "            date_times = list(date_times.values())\n",
    "            sorted_sigs = [x for _, x in sorted(zip(date_times, sigs))]\n",
    "            date_times = sorted(date_times)\n",
    "            # # argsort \n",
    "            # sorted_sigs = [x for _, x in sorted(zip(date_times, sigs))]\n",
    "            print(date_times, sorted_sigs)\n",
    "            ses_ct = 1\n",
    "            # print(sorted_sigs)\n",
    "            for ses in sorted_sigs:\n",
    "\n",
    "                if len(df_neurofunc[df_neurofunc['Session'] == ses]) == 0 and len(sorted_sigs) > 1:\n",
    "                    print('Session {} not found in df_neurofunc'.format(ses))\n",
    "                # else:\n",
    "                    # add date time column\n",
    "                df_neurofunc.loc[df_neurofunc['Session'] == ses, 'date_time'] = date_times[ses_ct - 1]\n",
    "                df_neurofunc.loc[df_neurofunc['Session'] == ses, 'ses_ct'] = ses_ct\n",
    "                ses_ct += 1\n",
    "\n",
    "print(df_neurofunc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neurofunc['ses_ct'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_neurofunc[df_neurofunc['date_time'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Session where date_time is NaT\n",
    "print(df_neurofunc[df_neurofunc['date_time'].isna()]['Session'].unique())\n",
    "print(len(df_neurofunc[df_neurofunc['date_time'].isna()]['Session'].unique()), len(df_neurofunc['Session'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_neurofunc.to_excel('object_complete.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to match neurofunc and remapping\n",
    "import re\n",
    "from scripts.batch_map.LEC_naming import LEC_naming_format, extract_name_lec\n",
    "\n",
    "def _check_single_format(filename, fformat, fxn):\n",
    "    if re.match(str(fformat), str(filename)) is not None:\n",
    "        return list(fxn(filename))\n",
    "\n",
    "# For neurofunc need to add extract, date, depth, name, stim \n",
    "# iterate thru rows of df_neurofunc and extract from signature\n",
    "for i, row in df_neurofunc.iterrows():\n",
    "    # extract\n",
    "    fname = row['Session']\n",
    "    \n",
    "    if 'CAGE' in fname:\n",
    "        # drop row\n",
    "        df_neurofunc.drop(i, inplace=True)\n",
    "    else:\n",
    "        print(fname)\n",
    "        group, name = extract_name_lec(fname)\n",
    "        formats = LEC_naming_format[group][name]['object']\n",
    "\n",
    "        for fformat in list(formats.keys()):\n",
    "            checked = _check_single_format(fname, fformat, formats[fformat])\n",
    "            if checked is not None:\n",
    "                break\n",
    "\n",
    "        stim, depth, name, date = checked\n",
    "        \n",
    "        df_neurofunc.at[i, 'date'] = date\n",
    "        df_neurofunc.at[i, 'depth'] = str(int(depth))\n",
    "        df_neurofunc.at[i, 'name'] = name\n",
    "        df_neurofunc.at[i, 'stim'] = stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neurofunc_row_identifiers = ['name','Tetrode', 'Cell ID', 'date', 'depth', 'stim']\n",
    "# remapping_row_identifiers = ['name','tetrode', 'unit_id', 'date', 'depth','object_location']\n",
    "\n",
    "# df_remapping_to_merge = df_remapping[df_remapping['score'] == 'whole']\n",
    "\n",
    "# for row in neurofunc_row_identifiers:\n",
    "#     df_neurofunc[row] = df_neurofunc[row].astype(str)\n",
    "# for row in remapping_row_identifiers:\n",
    "#     df_remapping_to_merge[row] = df_remapping_to_merge[row].astype(str)\n",
    "\n",
    "# matched_df = None\n",
    "# for i, row in df_remapping_to_merge.iterrows():\n",
    "#     # find row in df_neurofunc that mathces using identifiers\n",
    "#     for j, row2 in df_neurofunc.iterrows():\n",
    "#         matched = True\n",
    "#         for k in range(len(neurofunc_row_identifiers)):\n",
    "#             print('pair ' + str(neurofunc_row_identifiers[k]))\n",
    "#             print('new2')\n",
    "#             print(row2[neurofunc_row_identifiers[k]])\n",
    "#             print('new')\n",
    "#             print(row[remapping_row_identifiers[k]])\n",
    "            \n",
    "#             if row2[neurofunc_row_identifiers[k]] != row[remapping_row_identifiers[k]]:\n",
    "#                 matched = False\n",
    "\n",
    "#         # if row2[neurofunc_row_identifiers] == row[remapping_row_identifiers]:\n",
    "#         if matched == True:\n",
    "#             if matched_df is None:\n",
    "#                 row = pd.concat([row, row2], axis=0)\n",
    "#                 matched_df = row\n",
    "#             else:\n",
    "#                 row = pd.concat([row, row2], axis=0)\n",
    "#                 matched_df = pd.concat([matched_df, row], axis=1)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the common columns\n",
    "neurofunc_row_identifiers = ['name', 'Tetrode', 'Cell ID', 'date', 'depth', 'stim']\n",
    "remapping_row_identifiers = ['name', 'tetrode', 'unit_id', 'date', 'depth', 'object_location']\n",
    "\n",
    "for row in neurofunc_row_identifiers:\n",
    "    df_neurofunc[row] = df_neurofunc[row].astype(str)\n",
    "for row in remapping_row_identifiers:\n",
    "    df_remapping[row] = df_remapping[row].astype(str)\n",
    "df_neurofunc['Session'] = df_neurofunc['Session'].astype(str)\n",
    "df_remapping['signature'] = df_remapping['signature'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to store the merged rows\n",
    "merged_rows = []\n",
    "\n",
    "# Iterate through each row in df_remapping\n",
    "counter = 0\n",
    "for remapping_row in df_remapping.itertuples(index=False):\n",
    "    # Extract the identifiers from the remapping row\n",
    "    remapping_identifiers = [getattr(remapping_row, col) for col in remapping_row_identifiers]\n",
    "\n",
    "    # Find the matching row in df_neurofunc based on the identifiers\n",
    "    matching_row = df_neurofunc.loc[\n",
    "        (df_neurofunc[neurofunc_row_identifiers] == remapping_identifiers).all(axis=1)\n",
    "    ]\n",
    "\n",
    "    if len(matching_row) > 1:\n",
    "        # # pick row where neurofunc at column called 'Session' is the same as remapping at 'signature'\n",
    "        # neurofunc_row_identifiers = ['Session','name', 'Tetrode', 'Cell ID', 'date', 'depth', 'stim']\n",
    "        # remapping_row_identifiers = ['signature','name', 'tetrode', 'unit_id', 'date', 'depth', 'object_location']\n",
    "        # remapping_identifiers = [getattr(remapping_row, col) for col in remapping_row_identifiers]\n",
    "\n",
    "        # # Find the matching row in df_neurofunc based on the identifiers\n",
    "        # matching_row = df_neurofunc.loc[\n",
    "        #     (df_neurofunc[neurofunc_row_identifiers] == remapping_identifiers).all(axis=1)\n",
    "        # ]\n",
    "        # print('new')\n",
    "        # print(len(matching_row))\n",
    "        remapping_row_ses_ct = int(remapping_row[7].split('_')[-1])\n",
    "        # print(matching_row['ses_ct'] == remapping_row_ses_ct, matching_row['ses_ct'], remapping_row_ses_ct)\n",
    "        matching_row = matching_row[matching_row['ses_ct'] == remapping_row_ses_ct]\n",
    "\n",
    "\n",
    "    # Check if a matching row was found\n",
    "    if len(matching_row) == 0:\n",
    "        print(\"Found {} matching rows for {}\".format(len(matching_row), remapping_identifiers))\n",
    "        print(matching_row['Session'], getattr(remapping_row, 'signature'))\n",
    "        # stop()\n",
    "    elif len(matching_row) > 1:\n",
    "        # check all rows are matching on spatial info score \n",
    "        prev = None\n",
    "        for i, row in matching_row.iterrows():\n",
    "            if prev is None:\n",
    "                prev = row['spike_count']\n",
    "            else:\n",
    "                if prev != row['spike_count']:\n",
    "                    print('multiple matches')\n",
    "                    print(matching_row)\n",
    "                    # stop()\n",
    "        \n",
    "    # Join the matching rows\n",
    "    # merged_row = pd.concat([matching_row, pd.DataFrame([remapping_row], columns=df_remapping.columns)], axis=1)\n",
    "    merged_rows.append(matching_row)\n",
    "    counter += 1\n",
    "\n",
    "    if len(merged_rows) != counter:\n",
    "        print('error {} != {}'.format(len(merged_rows), counter))\n",
    "        print(matching_row)\n",
    "\n",
    "\n",
    "# Create the merged dataframe\n",
    "merged_df = pd.concat(merged_rows, ignore_index=True)\n",
    "\n",
    "# Print the merged dataframe\n",
    "# print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neurofunc.iloc[3130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remapping_identifiers = [getattr(remapping_row, col) for col in remapping_row_identifiers]\n",
    "\n",
    "    # Find the matching row in df_neurofunc based on the identifiers\n",
    "matching_row = df_neurofunc.loc[\n",
    "        (df_neurofunc[neurofunc_row_identifiers] == remapping_identifiers).all(axis=1)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remapping_identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neurofunc['ses_ct'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remapping['session_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_neurofunc.shape, df_remapping.shape, merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat df remapping and df merged \n",
    "df_full = pd.concat([df_remapping, merged_df], axis=1)\n",
    "df_full.to_excel('df_full_LEC.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat([df_remapping, merged_df], axis=1)\n",
    "# drop duplicate columns\n",
    "df_full = df_full.loc[:,~df_full.columns.duplicated()]\n",
    "\n",
    "dtemp = df_full[df_full['score'] == 'whole']\n",
    "lst = dtemp.groupby(['name', 'depth', 'date','tetrode', 'unit_id','session_id','object_location'])\n",
    "def _single(dtemp, grp):\n",
    "    if len(dtemp.groupby(['name', 'depth', 'date','tetrode', 'unit_id','session_id','object_location']).groups[grp]) != 1:\n",
    "        print(grp)\n",
    "        print(dtemp.groupby(['name', 'depth', 'date','tetrode', 'unit_id','session_id','object_location']).groups[grp])\n",
    "        # stop()\n",
    "\n",
    "list(map(lambda x: _single(dtemp, x), lst.groups))\n",
    "\n",
    "# fld_counts['field_count'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in merged_rows:\n",
    "    ln = len(row)\n",
    "    if ln != 1:\n",
    "        print(row['Session'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat df remapping and df merged \n",
    "df_full = pd.concat([df_remapping, merged_df], axis=1)\n",
    "df_full.to_excel('df_full_LEC.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPRISM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
