{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link: https://figshare.com/articles/dataset/Data_from_Method_of_place_cell_classification_determines_the_population_of_cells_identified/13560548\n",
    "import os \n",
    "\n",
    "pths = [r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\FrontiersRemappingData\\HC\\NikNak_20190827\\NikNak_20190827\",\n",
    "        r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\FrontiersRemappingData\\HC\\Popchips_20190827\\Popchips_20190827\",\n",
    "        r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\FrontiersRemappingData\\HC\\Cheeto_20190830\\Cheeto_20190830\",\n",
    "        r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\FrontiersRemappingData\\HC\\Cheeto_20190829\\Cheeto_20190829\",\n",
    "        r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\FrontiersRemappingData\\HC\\NikNak_20190826\\NikNak_20190826\",\n",
    "        r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\FrontiersRemappingData\\HC\\Popchips_20190827_1\\Popchips_20190827_1\",\n",
    "        r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\FrontiersRemappingData\\HC\\Voltorb_20190828\\Voltorb_20190828\",\n",
    "        r\"C:\\Users\\aaoun\\OneDrive - cumc.columbia.edu\\Desktop\\HussainiLab\\neuroscikit_test_data\\FrontiersRemappingData\\HC\\Voltorb_20190829\\Voltorb_20190829\",]        \n",
    "\n",
    "stacked_F = []\n",
    "stacked_spks = []\n",
    "\n",
    "for pth in pths:\n",
    "    loco_path = os.path.join(pth, 'locomotion.mat')\n",
    "    print(loco_path)\n",
    "    loco = loadmat(loco_path)\n",
    "    loco = loco['loco'] * 200\n",
    "\n",
    "    fall_path = os.path.join(pth, 'Fall.mat')\n",
    "    fall = loadmat(fall_path)\n",
    "    fall.keys()\n",
    "\n",
    "    F = fall['F']\n",
    "    Fneu = fall['Fneu']\n",
    "    spks = fall['spks']\n",
    "    iscell = fall['iscell']\n",
    "    valid_ids = np.where(iscell[:,0]==1)[0]\n",
    "    valid_probs = np.where(iscell[:,1]>0.5)[0]\n",
    "    valid_ids = np.array(valid_ids, dtype=int)\n",
    "    valid_probs = np.array(valid_probs, dtype=int)\n",
    "    valid_ids = [x for x in valid_ids if x in valid_probs]\n",
    "\n",
    "    F = F[valid_ids,:]\n",
    "    Fneu = Fneu[valid_ids,:]\n",
    "    spks = spks[valid_ids,:]\n",
    "    F = F - 0.7*Fneu\n",
    "    # for i in range(len(spks)):\n",
    "        # cpy = np.zeros(spks[i].shape)\n",
    "        # cpy[spks[i]>2.5*np.std(spks[i, :])] = 1 \n",
    "        # spks[i] = cpy\n",
    "\n",
    "    F = preprocess_data(F)\n",
    "\n",
    "\n",
    "    stacked_F.append(F[:, :19999])\n",
    "    stacked_spks.append(spks[:, :19999])\n",
    "\n",
    "F = np.vstack(stacked_F)\n",
    "spks = np.vstack(stacked_spks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_data(data, window_size=int(15*7.5), threshold_multiplier=2, end_threshold_multiplier=0.5):\n",
    "    \"\"\"\n",
    "    Preprocesses the input data using the modified method.\n",
    "\n",
    "    Parameters:\n",
    "        data (numpy.ndarray): Input time series data.\n",
    "        window_size (int): Size of the window in seconds for baseline calculation.\n",
    "        threshold_multiplier (float): Multiplier for standard deviation to identify significant transients.\n",
    "        end_threshold_multiplier (float): Multiplier for standard deviation to identify the end of transients.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Preprocessed data.\n",
    "    \"\"\"\n",
    "    num_samples, num_cells = data.shape\n",
    "    preprocessed_data = np.zeros((num_samples, num_cells))\n",
    "    \n",
    "    windows_processed_data = []\n",
    "    \n",
    "    for i in range(0, num_samples, window_size):\n",
    "        window_data = data[i:i + window_size, :]\n",
    "        \n",
    "        # Calculate baseline as the 8th percentile within the window\n",
    "        baseline = np.percentile(window_data, 8, axis=0)\n",
    "        \n",
    "        # Handle case where baseline is 0 to prevent division by zero\n",
    "        # baseline[baseline == 0] = 1\n",
    "        \n",
    "        # Normalize cell trace by subtracting the baseline\n",
    "        normalized_data = window_data - baseline.reshape(1, -1)\n",
    "        \n",
    "        # Store processed data for this window\n",
    "        windows_processed_data.append(normalized_data)\n",
    "    \n",
    "    normalized_data = np.vstack(windows_processed_data)\n",
    "    # Identify significant transients for the entire trace of each cell\n",
    "    start_condition = normalized_data > threshold_multiplier * np.std(normalized_data, axis=0)\n",
    "    end_condition = normalized_data < -end_threshold_multiplier * np.std(normalized_data, axis=0)\n",
    "    # significant_events = start_condition & end_condition\n",
    "\n",
    "    # # Set fluorescence outside of significant events to 0\n",
    "    # preprocessed_data = np.where(significant_events, normalized_data, 0)\n",
    "\n",
    "    # Combine start and end conditions to identify significant events\n",
    "    significant_events = np.zeros_like(normalized_data)\n",
    "    event_started = False\n",
    "    \n",
    "    for j in range(num_cells):\n",
    "        for t in range(normalized_data.shape[0]):\n",
    "            if start_condition[t, j]:\n",
    "                event_started = True\n",
    "            if event_started:\n",
    "                significant_events[t, j] = 1\n",
    "            if end_condition[t, j] and event_started:\n",
    "                event_started = False\n",
    "    \n",
    "    # Set fluorescence outside of significant events to 0\n",
    "    preprocessed_data = np.where(significant_events, normalized_data, 0)\n",
    "    \n",
    "    \n",
    "    return preprocessed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For each ROI, fluorescence time courses were normalised to baseline fluorescence by dividing the whole trace by the average intensity in that ROI during the first 100 frames of the recording.\n",
    "# F = F / np.mean(F[:,:100], axis=1)[:,None]\n",
    "# # drop rows average F is too large\n",
    "# keep_F = []\n",
    "# for f in F:\n",
    "#     if np.max(f) > 1000 or np.min(f) < -400:\n",
    "#         pass\n",
    "#     else:\n",
    "#         keep_F.append(f)\n",
    "\n",
    "# keep_F = np.array(keep_F)\n",
    "# # divide by the baseline, which was defined as the 8th percentile of values in each ~15 second interval\n",
    "inf_contents = []\n",
    "keep_spks = []\n",
    "keep_spks_order = []\n",
    "max_line = []\n",
    "keep_spks_unchanged = []\n",
    "for spk in spks:\n",
    "    # idx_event = np.where(spk >= 3*np.std(spk))[0]\n",
    "    # new_spk = np.zeros_like(spk)\n",
    "    # new_spk[idx_event] = 1\n",
    "    # frames_to_sum = 150\n",
    "    # total_sums = len(new_spk) // frames_to_sum\n",
    "    # new_spk = new_spk[:total_sums * frames_to_sum].reshape(-1, frames_to_sum)\n",
    "    # new_spk = np.mean(new_spk, axis=1)\n",
    "    # keep_spks.append(new_spk)\n",
    "    # keep_spks_order.append(np.argmax(spk))\n",
    "\n",
    "    # if np.max(spk) > 10000:\n",
    "    #     pass\n",
    "    if True:\n",
    "        # print('new')\n",
    "        # print(np.mean(spk[:100]), np.max(spk))\n",
    "        # spk = spk / np.mean(spk[:100])\n",
    "        # spk = spk.squeeze()\n",
    "        # print(np.mean(spk), np.max(spk))\n",
    "        # if np.max(spk) < 1000 and np.min(spk) > 0:\n",
    "        if True:\n",
    "            spk_copy = np.copy(spk)\n",
    "            frames_to_sum = int(7.5*15)\n",
    "            total_sums = len(spk) // frames_to_sum\n",
    "            spk = spk[:total_sums * frames_to_sum].reshape(-1, frames_to_sum)\n",
    "            spk = np.mean(spk, axis=1)/15\n",
    "            if np.min(spk) > 0 and np.mean(spk) > 0.1:\n",
    "                # and np.max(spk) < 1000 and np.mean(spk) > 0.1:\n",
    "                spk = spk / np.mean(spk)\n",
    "\n",
    "                # mean_rate = np.mean(spk)\n",
    "                # log_argument = spk / mean_rate\n",
    "                # log_argument[log_argument < 1] = 1\n",
    "                # inf_rate = np.ma.sum(spk * np.ma.log2(log_argument))\n",
    "                # inf_content = inf_rate / mean_rate\n",
    "\n",
    "                # inf_contents.append(inf_content)\n",
    "                keep_spks_unchanged.append(spk_copy)\n",
    "                keep_spks.append(spk)\n",
    "                keep_spks_order.append(np.argmax(spk))\n",
    "                index_at_max = np.argmax(spk)\n",
    "                position_at_max = index_at_max * 200 / len(spk)\n",
    "                max_line.append(position_at_max)\n",
    "\n",
    "keep_spks = np.array(keep_spks)[np.argsort(keep_spks_order)[::-1]]\n",
    "# plt.imshow(keep_spks, aspect='auto', cmap='jet')\n",
    "# plt.show()\n",
    "# stop()\n",
    "\n",
    "\n",
    "max_line = np.array(max_line)[np.argsort(keep_spks_order)[::-1]]\n",
    "reference = np.zeros_like(keep_spks[0])\n",
    "# reference = np.mean(keep_spks[0])\n",
    "midpoint = len(reference)//2\n",
    "# reference_midpoint = np.copy(reference)\n",
    "# reference_midpoint[midpoint-1:midpoint+1] = 1\n",
    "\n",
    "ref_start = np.zeros_like(keep_spks[0])\n",
    "ref_start[0] = 1\n",
    "\n",
    "# ref_start_narrow = np.zeros_like(keep_spks[0])\n",
    "# ref_start_narrow[0:5] = 1\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "ref_start_narrow =  gaussian_filter1d(ref_start, sigma=5)\n",
    "ref_start_narrow = ref_start_narrow / np.mean(ref_start_narrow)\n",
    "# plt.plot(ref_start_narrow)\n",
    "# plt.show()\n",
    "# stop()\n",
    "\n",
    "ref_start_wide = np.zeros_like(keep_spks[0])\n",
    "ref_start_wide[0:10] = 1\n",
    "\n",
    "ref_end = np.zeros_like(keep_spks[0])\n",
    "ref_end[midpoint] = 1\n",
    "\n",
    "# ref_end_narrow = np.zeros_like(keep_spks[0])\n",
    "# ref_end_narrow[midpoint-2:midpoint+3] = 1\n",
    "ref_end_narrow = gaussian_filter1d(ref_end, sigma=5/2)\n",
    "ref_end_narrow = ref_end_narrow / np.mean(ref_end_narrow)\n",
    "\n",
    "ref_end_wide = np.zeros_like(keep_spks[0])\n",
    "ref_end_wide[midpoint-5:midpoint+5] = 1\n",
    "\n",
    "positions = np.arange(0, 200, 200/len(ref_start))\n",
    "# average of first 10 cells\n",
    "# avg = np.mean(keep_spks[:10], axis=0)\n",
    "count = 0\n",
    "for reference in [ref_start, ref_start_narrow, ref_start_wide, ref_end, ref_end_narrow, ref_end_wide]:\n",
    "# ref1 = keep_spks[0]\n",
    "# ref2 = keep_spks[1]\n",
    "# ref3 = keep_spks[2]\n",
    "\n",
    "# ref4 = np.mean(keep_spks[:10], axis=0)\n",
    "# ref5 = np.mean(keep_spks[:100], axis=0)\n",
    "# ref6 = np.mean(keep_spks[:], axis=0)\n",
    "\n",
    "# for reference in [ref1, ref2, ref3, ref4, ref5, ref6]:\n",
    "    reference = reference / np.mean(reference) \n",
    "    emds = []\n",
    "    corrs = []\n",
    "    from scipy.stats import pearsonr, spearmanr\n",
    "    from scipy.stats import wasserstein_distance\n",
    "    count2 = 0\n",
    "    for spk in keep_spks:\n",
    "\n",
    "        # first_half = spk[:len(spk)//2]\n",
    "        # second_half = spk[len(spk)//2:]\n",
    "\n",
    "        # corrs.append(np.corrcoef(reference, spk)[0,1])\n",
    "\n",
    "        r, p = pearsonr(reference, spk)\n",
    "        # ref_unchanged = np.zeros(len(keep_spks_unchanged[count]))\n",
    "        # ref_unchanged[0] = 1\n",
    "        # r, p = pearsonr(ref_unchanged, keep_spks_unchanged[count])\n",
    "        \n",
    "        # r, p = spearmanr(first_half, second_half)\n",
    "        corrs.append(r)\n",
    "        \n",
    "        emd = wasserstein_distance(reference, spk, positions, positions)\n",
    "        # emd = wasserstein_distance(first_half, second_half, positions[:len(spk)//2], positions[len(spk)//2:])\n",
    "        emds.append(emd)\n",
    "        count2 += 1\n",
    "\n",
    "    fig = plt.figure(figsize=(25,5))\n",
    "\n",
    "    ax = fig.add_subplot(1, 4, 1)\n",
    "                    \n",
    "    img = ax.imshow(keep_spks, aspect='auto', cmap='jet', extent=[0, 25, 0, len(keep_spks)])\n",
    "    ax.set_title('Linear track firing rates', fontsize=12)\n",
    "\n",
    "    ax1 = ax\n",
    "    \n",
    "    ax.set_ylabel('Cell #',fontsize=12)\n",
    "    ax.set_xlabel('Position (cm)',fontsize=12)\n",
    "    cbar = plt.colorbar(img, ax=ax, label='dF/F')\n",
    "    cbar.set_label('dF/F', fontsize=12)\n",
    "\n",
    "    ax = fig.add_subplot(1, 4, 2)\n",
    "    \n",
    "    l1 = ax.plot(np.flip(emds), np.arange(0,len(emds)),color='red')\n",
    "    window_size = int(0.1*len(emds))\n",
    "    smooth_trace_emd = np.convolve(np.flip(emds), np.ones(window_size)/window_size, mode='valid')\n",
    "    l2 = ax.plot(smooth_trace_emd, np.arange(0,len(emds),len(emds)/len(smooth_trace_emd)),'k:',lw=4)\n",
    "    # l2 = ax.plot(np.sort(np.flip(emds)), np.arange(0,len(emds)),'k:',lw=4)\n",
    "    ax.set_ylabel('Cell #',fontsize=12)\n",
    "    ax.set_xlabel('EMD',fontsize=12)\n",
    "    # axt2 = ax.twinx().twiny()\n",
    "    # axt2.set_ylabel('Cell #',fontsize=12)\n",
    "    # axt2.set_xlabel('Position (cm)',fontsize=12)\n",
    "\n",
    "    # ax2 = axt2\n",
    "\n",
    "    # if count >= 3:\n",
    "    # l3 = axt2.plot(np.flip(max_line), np.arange(0, len(max_line)), color='black')\n",
    "    #     axt2.set_xlim(axt2.get_xlim()[::-1])\n",
    "    # else:\n",
    "    #     l3 = axt2.plot(np.flip(max_line), np.arange(0, len(max_line)), color='black')\n",
    "        \n",
    "    legend_properties = {'size': 12}\n",
    "\n",
    "    # if count >= 3:\n",
    "    ax.legend([l1[0], l2[0]], ['EMD', 'Sorted EMD'], loc='upper left', prop=legend_properties)\n",
    "    # else:\n",
    "    #     # ax.set_xlim(0)\n",
    "    #     ax.legend([l1[0], l2[0]], ['EMD', 'Sorted EMD'], loc ='upper left', bbox_to_anchor=(0.1, 1), prop=legend_properties)\n",
    "\n",
    "    ax = fig.add_subplot(1, 4, 3)\n",
    "    l1 = ax.plot(np.flip(corrs), np.arange(0,len(corrs)),color='blue')\n",
    "    window_size = int(0.1*len(corrs))\n",
    "    smooth_trace_corr = np.convolve(corrs, np.ones(window_size)/window_size, mode='valid')\n",
    "    smooth_trace_corr = np.flip(smooth_trace_corr)\n",
    "    l2 = ax.plot(smooth_trace_corr, np.arange(0,len(corrs),len(corrs)/len(smooth_trace_corr)),'k:',lw=4)\n",
    "\n",
    "    # flip x axis\n",
    "    ax.invert_xaxis()\n",
    "    ax.set_ylabel('Cell #',fontsize=12)\n",
    "    ax.set_xlabel('Correlation',fontsize=12)\n",
    "    # axt2 = ax.twinx().twiny()\n",
    "    # axt2.set_ylabel('Cell #',fontsize=12)\n",
    "    # axt2.set_xlabel('Position (cm)',fontsize=12)\n",
    "    # if count >= 3:\n",
    "    # l3 = axt2.plot(np.flip(max_line), np.arange(0, len(max_line)), color='black')\n",
    "    #     axt2.set_xlim(axt2.get_xlim()[::-1])\n",
    "    # else:\n",
    "    #     l3 = axt2.plot(np.flip(max_line), np.arange(0, len(max_line)), color='black')\n",
    "        \n",
    "    legend_properties = {'size': 12}\n",
    "\n",
    "    # if count >= 3:\n",
    "    ax.legend([l1[0], l2[0]], ['Correlation', 'Running Avg'], loc='upper left', prop=legend_properties)\n",
    "    # else:\n",
    "    #     ax.legend([l1[0], l2[0]], ['Correlation', 'Running Avg'], loc ='upper left', bbox_to_anchor=(0.1, 1), prop=legend_properties) \n",
    "    \n",
    "    # ax3 = axt2\n",
    "\n",
    "    ax = fig.add_subplot(1, 4, 4)\n",
    "    lbl_max = ax.plot(np.flip(max_line), np.arange(0, len(max_line)), color='black')\n",
    "    ax_emd = ax.twinx().twiny()\n",
    "    lbl_emd = ax_emd.plot(smooth_trace_emd, np.arange(0,len(emds),len(emds)/len(smooth_trace_emd)), color='red')\n",
    "    # hide x and y ticks\n",
    "    ax_emd.set_xticks([])\n",
    "    ax_emd.set_yticks([])\n",
    "    ax_corr = ax.twinx().twiny()\n",
    "    lbl_corr = ax_corr.plot(smooth_trace_corr, np.arange(0,len(corrs),len(corrs)/len(smooth_trace_corr)), color='blue')\n",
    "    # hide x and y ticks\n",
    "    ax_corr.set_xticks([])\n",
    "    ax_corr.set_yticks([])\n",
    "    ax_corr.invert_xaxis()\n",
    "\n",
    "    if count == 1:\n",
    "        ax_again = ax.twinx().twiny()\n",
    "        toplot = ref_start_narrow.reshape(-1,1)\n",
    "        # toplot[toplot< 0.001] = np.nan\n",
    "        ax_again.imshow(toplot.T, cmap='jet', aspect='auto', alpha=0.5)\n",
    "        ax_again.set_xticks([])\n",
    "        ax_again.set_yticks([])\n",
    "    elif count == 4:\n",
    "        ax_again = ax.twinx().twiny()\n",
    "        toplot = ref_end_narrow.reshape(-1,1)\n",
    "        # toplot[toplot< 0.001] = np.nan\n",
    "        ax_again.imshow(toplot.T, cmap='jet', aspect='auto', alpha=0.5)\n",
    "        ax_again.set_xticks([])\n",
    "        ax_again.set_yticks([])\n",
    "        \n",
    "\n",
    "\n",
    "    ax.set_ylabel('Cell #',fontsize=12)\n",
    "    ax.set_xlabel('Position (cm)',fontsize=12)\n",
    "\n",
    "    for ax in [ax,]:\n",
    "        if count == 0:\n",
    "            ax.vlines(positions[0], 0, len(keep_spks), color='k', linestyle='--', lw=2)\n",
    "        # elif count == 1:\n",
    "        #     ax.vlines(positions[0], 0, len(keep_spks), color='k', linestyle='--', lw=2)\n",
    "        #     ax.vlines(positions[5], 0, len(keep_spks), color='k', linestyle='--', lw=2)\n",
    "        elif count == 2:\n",
    "            ax.vlines(positions[0], 0, len(keep_spks), color='k', linestyle='--', lw=2)\n",
    "            ax.vlines(positions[10], 0, len(keep_spks), color='k', linestyle='--', lw=2)\n",
    "        elif count == 3:\n",
    "            ax.vlines(positions[midpoint], 0, len(keep_spks), color='k', linestyle='--', lw=2)\n",
    "        # elif count == 4:\n",
    "        #     ax.vlines(positions[midpoint-2], 0, len(keep_spks), color='k', linestyle='--', lw=2)\n",
    "        #     ax.vlines(positions[midpoint+3], 0, len(keep_spks), color='k', linestyle='--', lw=2)\n",
    "\n",
    "        elif count == 5:\n",
    "            ax.vlines(positions[midpoint-5], 0, len(keep_spks), color='k', linestyle='--', lw=2)\n",
    "            ax.vlines(positions[midpoint+5], 0, len(keep_spks), color='k', linestyle='--', lw=2)\n",
    "    # legend\n",
    "    legend_properties = {'size': 12}\n",
    "    if count >= 3:\n",
    "        ax.legend([lbl_max[0], lbl_emd[0], lbl_corr[0]], ['Max', 'EMD', 'Correlation'], loc='upper left', prop=legend_properties)\n",
    "    else:\n",
    "        ax.legend([lbl_max[0], lbl_emd[0], lbl_corr[0]], ['Max', 'EMD', 'Correlation'], loc ='upper left', bbox_to_anchor=(0.1, 1), prop=legend_properties)\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(keep_spks), len(F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(ref_end_narrow.reshape(1,-1), aspect='auto', cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ratemap = np.random.rand(32, 32)\n",
    "# smooth the ratemap\n",
    "from scipy.ndimage import gaussian_filter\n",
    "ratemap = gaussian_filter(ratemap, sigma=2)\n",
    "ratemap = ratemap / np.max(ratemap)\n",
    "\n",
    "plt.imshow(ratemap, cmap='jet')\n",
    "plt.show()\n",
    "# Assuming your ratemap is called ratemap and has shape (32, 32)\n",
    "mask_radius = 25  # Adjust the radius of the circular mask as needed\n",
    "mask_center = (mask_radius, mask_radius)\n",
    "mask = np.ones((32, 32))\n",
    "y, x = np.ogrid[-mask_center[0]:mask.shape[0]-mask_center[0], -mask_center[1]:mask.shape[1]-mask_center[1]]\n",
    "mask_circle = x*x + y*y <= mask_radius*mask_radius\n",
    "mask[mask_circle] = 0\n",
    "\n",
    "ratemap[mask.astype(bool)] = np.nan\n",
    "plt.imshow(ratemap, cmap='jet')\n",
    "plt.show()\n",
    "\n",
    "shift_center = (5, 5)  # Adjust the shift values as needed\n",
    "shifted_ratemap = np.roll(ratemap, shift=shift_center, axis=(0, 1))\n",
    "shifted_ratemap[mask.astype(bool)] = np.nan\n",
    "\n",
    "plt.imshow(shifted_ratemap, cmap='jet')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(spk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(3, 3, i+1)\n",
    "    img = ax.imshow(keep_spks[100*i:100*(i+1)], aspect='auto', cmap='jet', extent=[0,200,100*i,100*(i+1)])\n",
    "    plt.colorbar(img, ax=ax)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make grid of alternating columns of 0s and 1s\n",
    "sze = 8\n",
    "\n",
    "g1 = np.zeros((sze, sze))\n",
    "for i in range(sze):\n",
    "    if i % 2 == 0:\n",
    "        g1[:,i] = 1\n",
    "\n",
    "g2 = np.zeros((sze, sze))\n",
    "for i in range(sze):\n",
    "    if i % 2 != 0:\n",
    "        g2[:,i] = 1\n",
    "\n",
    "# 2d positions\n",
    "positions = []\n",
    "for i in range(sze):\n",
    "    for j in range(sze):\n",
    "        positions.append([i,j])\n",
    "# print(positions.shape, g1.shape, g2.shape)\n",
    "emd = wasserstein_distance(g1.flatten(), g1.flatten(), positions, positions)\n",
    "pearson = np.corrcoef(g1.flatten(), g1.flatten())[0,1]\n",
    "emd = round(emd, 2)\n",
    "pearson = round(pearson, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(7,3))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "img = ax.imshow(g1, aspect='auto', cmap='jet')\n",
    "plt.colorbar(img, ax=ax)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "img = ax.imshow(g1, aspect='auto', cmap='jet')\n",
    "plt.colorbar(img, ax=ax)\n",
    "\n",
    "fig.suptitle('EMD: ' + str(emd) + ' Pearson: ' + str(pearson))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "emd = wasserstein_distance(g1.flatten(), g2.flatten(), positions, positions)\n",
    "pearson = np.corrcoef(g1.flatten(), g2.flatten())[0,1]\n",
    "emd = round(emd, 2)\n",
    "pearson = round(pearson, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(7,3))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "img = ax.imshow(g1, aspect='auto', cmap='jet')\n",
    "plt.colorbar(img, ax=ax)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "img = ax.imshow(g2, aspect='auto', cmap='jet')\n",
    "plt.colorbar(img, ax=ax)\n",
    "\n",
    "fig.suptitle('EMD: ' + str(emd) + ' Pearson: ' + str(pearson))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# rotate g1\n",
    "g1_rot = np.rot90(g1)\n",
    "emd = wasserstein_distance(g1_rot.flatten(), g1.flatten(), positions, positions)\n",
    "pearson = np.corrcoef(g1_rot.flatten(), g1.flatten())[0,1]\n",
    "emd = round(emd, 2)\n",
    "pearson = round(pearson, 17)\n",
    "\n",
    "fig = plt.figure(figsize=(7,3))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "img = ax.imshow(g1, aspect='auto', cmap='jet')\n",
    "plt.colorbar(img, ax=ax)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "img = ax.imshow(g1_rot, aspect='auto', cmap='jet')\n",
    "plt.colorbar(img, ax=ax)\n",
    "\n",
    "fig.suptitle('EMD: ' + str(emd) + ' Pearson: ' + str(pearson))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# rotate g2\n",
    "g2_rot = np.rot90(g2)\n",
    "emd = wasserstein_distance(g2_rot.flatten(), g1.flatten(), positions, positions)\n",
    "pearson = np.corrcoef(g2_rot.flatten(), g1.flatten())[0,1]\n",
    "emd = round(emd, 2)\n",
    "pearson = round(pearson, 17)\n",
    "\n",
    "fig = plt.figure(figsize=(7,3))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "img = ax.imshow(g1, aspect='auto', cmap='jet')\n",
    "plt.colorbar(img, ax=ax)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "img = ax.imshow(g2_rot, aspect='auto', cmap='jet')\n",
    "plt.colorbar(img, ax=ax)\n",
    "\n",
    "fig.suptitle('EMD: ' + str(emd) + ' Pearson: ' + str(pearson))\n",
    "\n",
    "fig.tight_layout()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPRISM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
